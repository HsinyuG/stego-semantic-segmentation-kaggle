{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 1. git clone"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-07T14:13:24.633263Z","iopub.status.busy":"2024-04-07T14:13:24.632885Z","iopub.status.idle":"2024-04-07T14:13:25.788507Z","shell.execute_reply":"2024-04-07T14:13:25.787750Z","shell.execute_reply.started":"2024-04-07T14:13:24.633231Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","!df -h /kaggle/input\n","!ls\n","!pip list | grep torch\n","# ensure pytorch-lightning==1.5.10"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T09:11:14.568139Z","iopub.status.busy":"2024-03-30T09:11:14.567606Z","iopub.status.idle":"2024-03-30T09:11:17.221222Z","shell.execute_reply":"2024-03-30T09:11:17.220093Z","shell.execute_reply.started":"2024-03-30T09:11:14.568095Z"},"trusted":true},"outputs":[],"source":["!git clone https://github.com/mhamilton723/STEGO.git"]},{"cell_type":"markdown","metadata":{},"source":["### 2. preparation and environment"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-07T14:13:42.307882Z","iopub.status.busy":"2024-04-07T14:13:42.307160Z","iopub.status.idle":"2024-04-07T14:16:04.077393Z","shell.execute_reply":"2024-04-07T14:16:04.076258Z","shell.execute_reply.started":"2024-04-07T14:13:42.307849Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# python environment\n","!pip install wget\n","!pip install torchmetrics\n","!pip install hydra-core\n","!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n","!pip install omegaconf\n","# !pip install pytorch-lightning==1.8.4\n","!pip install pytorch-lightning==1.5.10"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-07T14:22:59.326268Z","iopub.status.busy":"2024-04-07T14:22:59.325884Z","iopub.status.idle":"2024-04-07T14:23:02.171676Z","shell.execute_reply":"2024-04-07T14:23:02.170663Z","shell.execute_reply.started":"2024-04-07T14:22:59.326241Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# change path\n","!pwd\n","!ls\n","import os\n","from os.path import join\n","os.chdir(\"/kaggle/working/STEGO/src\")\n","saved_models_dir = join(\"..\", \"saved_models\")\n","os.makedirs(saved_models_dir, exist_ok=True)\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T09:56:51.484263Z","iopub.status.busy":"2024-03-30T09:56:51.483712Z","iopub.status.idle":"2024-03-30T09:59:13.966277Z","shell.execute_reply":"2024-03-30T09:59:13.965059Z","shell.execute_reply.started":"2024-03-30T09:56:51.484221Z"},"trusted":true},"outputs":[],"source":["# optional, only necessary if you want to evaluate the trained models provided by the paper\n","!python download_models.py"]},{"cell_type":"markdown","metadata":{},"source":["### 3. configuration files"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T17:07:12.253921Z","iopub.status.busy":"2024-04-07T17:07:12.252834Z","iopub.status.idle":"2024-04-07T17:07:12.264419Z","shell.execute_reply":"2024-04-07T17:07:12.263371Z","shell.execute_reply.started":"2024-04-07T17:07:12.253875Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile configs/eval_config.yml\n","output_root: '../'\n","pytorch_data_dir: '../datadrive/pytorch-data/'\n","# pytorch_data_dir: '/kaggle/input/potsdam/'\n","# experiment_name: \"cocostuff\"\n","# experiment_name: \"potsdam\"\n","experiment_name: \"voc\"\n","res: 320\n","batch_size: 8\n","num_workers: 3\n","run_picie: True\n","run_crf: True\n","run_prediction: True\n","dark_mode: True\n","use_ddp: False\n","\n","model_paths:\n","#   - \"../saved_models/cocostuff27_vit_base_5.ckpt\"\n","  #- \"../saved_models/cityscapes_vit_base_1.ckpt\"\n","#   - \"../saved_models/potsdam_test.ckpt\"\n","  - \"../saved_models/epoch=108-step=4999.ckpt\"\n","\n","hydra:\n","  run:\n","    dir: \".\"\n","  output_subdir: ~\n","  #job_logging: \"disabled\"\n","  #hydra_logging: \"disabled\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T14:24:47.697085Z","iopub.status.busy":"2024-04-07T14:24:47.696731Z","iopub.status.idle":"2024-04-07T14:24:47.704471Z","shell.execute_reply":"2024-04-07T14:24:47.703598Z","shell.execute_reply.started":"2024-04-07T14:24:47.697056Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile configs/train_config.yml\n","output_root: '../'\n","pytorch_data_dir: '../datadrive/pytorch-data/'\n","experiment_name: \"exp1\"\n","# log_dir: \"cocostuff27\"\n","# log_dir: \"potsdam\"\n","log_dir: \"voc\"\n","azureml_logging: False\n","submitting_to_aml: False\n","\n","# Loader params\n","num_workers: 3 # 24\n","max_steps: 5000\n","batch_size: 16\n","\n","num_neighbors: 7\n","# dataset_name: \"cocostuff27\"\n","# dataset_name: \"potsdam\"\n","dataset_name: \"voc\"\n","\n","# Used if dataset_name is \"directory\"\n","dir_dataset_name: ~\n","dir_dataset_n_classes: 5\n","\n","has_labels: False\n","# crop_type: \"five\"\n","crop_type: ~\n","crop_ratio: .5\n","res: 224\n","loader_crop_type: \"center\"\n","\n","# Model Params\n","extra_clusters: 0\n","use_true_labels: False\n","use_recalibrator: False\n","model_type: \"vit_small\"\n","arch: \"dino\"\n","use_fit_model: False\n","dino_feat_type: \"feat\"\n","projection_type: \"nonlinear\"\n","#projection_type: linear\n","dino_patch_size: 8\n","granularity: 1\n","continuous: True\n","dim: 70\n","dropout: True\n","zero_clamp: True\n","\n","lr: 5e-4\n","pretrained_weights: ~\n","use_salience: False\n","stabalize: False\n","stop_at_zero: True\n","\n","# Feature Contrastive params\n","pointwise: True\n","feature_samples: 11\n","neg_samples: 5\n","aug_alignment_weight: 0.0\n","\n","correspondence_weight: 1.0\n","\n","\n","# IAROA vit small 1/31/22\n","neg_inter_weight: 0.63\n","pos_inter_weight: 0.25\n","pos_intra_weight: 0.67\n","neg_inter_shift: 0.46\n","pos_inter_shift: 0.12\n","pos_intra_shift: 0.18\n","\n","# Potsdam vit small 1/31/22\n","#neg_inter_weight: 0.63\n","#pos_inter_weight: 0.25\n","#pos_intra_weight: 0.67\n","#neg_inter_shift: 0.46\n","#pos_inter_shift: 0.02\n","#pos_intra_shift: 0.08\n","\n","# Cocostuff27 vit small 1/31/22\n","#neg_inter_weight: 0.63\n","#pos_inter_weight: 0.25\n","#pos_intra_weight: 0.67\n","#neg_inter_shift: 0.66\n","#pos_inter_shift: 0.02\n","#pos_intra_shift: 0.08\n","\n","\n","## Cocostuff27 10/3 vit_base\n","\n","#neg_inter_weight: 0.1538476246415498\n","#pos_inter_weight: 1\n","#pos_intra_weight: 0.1\n","#\n","#neg_inter_shift: 1\n","#pos_inter_shift: 0.2\n","#pos_intra_shift: 0.12\n","\n","\n","## Cocostuff27 10/3 vit_small\n","#neg_inter_weight: .63\n","#pos_inter_weight: .25\n","#pos_intra_weight: .67\n","#\n","#neg_inter_shift: .16\n","#pos_inter_shift: .02\n","#pos_intra_shift: .08\n","\n","\n","\n","## Cocostuff27 10/3 moco\n","#neg_inter_weight: .63\n","#pos_inter_weight: .25\n","#pos_intra_weight: .67\n","#\n","#neg_inter_shift: .26\n","#pos_inter_shift: .36\n","#pos_intra_shift: .32\n","\n","#pos_inter_shift: .12\n","#pos_intra_shift: .18\n","\n","## Cocostuff27\n","#neg_inter_weight: .72\n","#pos_inter_weight: .80\n","#pos_intra_weight: .29\n","#\n","#neg_inter_shift: .86\n","#pos_inter_shift: .04\n","#pos_intra_shift: .34\n","\n","# Cityscapes 10/3\n","\n","#neg_inter_weight: 0.9058762625226623\n","#pos_inter_weight: 0.577453483136995\n","#pos_intra_weight: 1\n","#\n","#neg_inter_shift: 0.31361241889448443\n","#pos_inter_shift: 0.1754346515479633\n","#pos_intra_shift: 0.45828472207\n","\n","\n","# Cityscapes\n","#neg_inter_weight: .72\n","#pos_inter_weight: .18\n","#pos_intra_weight: .46\n","#\n","#neg_inter_shift: .25\n","#pos_inter_shift: .20\n","#pos_intra_shift: .25\n","\n","\n","rec_weight: 0.0\n","repulsion_weight: 0.0\n","\n","# CRF Params\n","crf_weight: 0.0\n","alpha: .5\n","beta: .15\n","gamma: .05\n","w1: 10.0\n","w2: 3.0\n","shift: 0.00\n","crf_samples: 1000\n","color_space: \"rgb\"\n","\n","reset_probe_steps: ~\n","\n","# Logging params\n","n_images: 5\n","scalar_log_freq: 10\n","checkpoint_freq: 50\n","val_freq: 100\n","hist_freq: 100\n","\n","\n","hydra:\n","  run:\n","    dir: \".\"\n","  output_subdir: ~\n","  #job_logging: \"disabled\"\n","  #hydra_logging: \"disabled\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile configs/plot_config.yml\n","output_root: '../'\n","pytorch_data_dir: '../datadrive/pytorch-data/'\n","experiment_name: \"exp1\"\n","log_dir: \"cleaning\"\n","\n","plot_correspondence: True\n","plot_movie: True\n","\n","\n","# Loader params\n","num_workers: 24\n","max_steps: 5000\n","\n","num_neighbors: 7\n","\n","batch_size: 16\n","# dataset_name: \"cocostuff27\"\n","dataset_name: \"potsdam\"\n","crop_type: ~\n","crop_ratio: .5\n","res: 224\n","loader_crop_type: \"center\"\n","\n","# Model Params\n","extra_clusters: 0\n","use_true_labels: False\n","use_recalibrator: False\n","model_type: \"vit_small\"\n","arch: \"dino\"\n","use_fit_model: False\n","dino_feat_type: \"feat\"\n","projection_type: \"nonlinear\"\n","dino_patch_size: 8\n","granularity: 1\n","continuous: True\n","dim: 70\n","dropout: True\n","zero_clamp: True\n","\n","\n","lr: 5e-4\n","pretrained_weights: ~\n","use_salience: False\n","stabalize: False\n","stop_at_zero: True\n","\n","hydra:\n","  run:\n","    dir: \".\"\n","  output_subdir: ~\n","  #job_logging: \"disabled\"\n","  #hydra_logging: \"disabled\"\n"]},{"cell_type":"markdown","metadata":{},"source":["### 4. scripts"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T14:24:54.804066Z","iopub.status.busy":"2024-04-07T14:24:54.803752Z","iopub.status.idle":"2024-04-07T14:24:54.818158Z","shell.execute_reply":"2024-04-07T14:24:54.817191Z","shell.execute_reply.started":"2024-04-07T14:24:54.804043Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile utils.py\n","import collections\n","import os\n","from os.path import join\n","import io\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.multiprocessing\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import wget\n","from PIL import Image\n","from scipy.optimize import linear_sum_assignment\n","# from torch._six import string_classes\n","string_classes = str\n","from torch.utils.data import DataLoader\n","from torch.utils.data._utils.collate import np_str_obj_array_pattern, default_collate_err_msg_format\n","from torchmetrics import Metric\n","from torchvision import models\n","from torchvision import transforms as T\n","from torch.utils.tensorboard.summary import hparams\n","\n","\n","def prep_for_plot(img, rescale=True, resize=None):\n","    if resize is not None:\n","        img = F.interpolate(img.unsqueeze(0), resize, mode=\"bilinear\")\n","    else:\n","        img = img.unsqueeze(0)\n","\n","    plot_img = unnorm(img).squeeze(0).cpu().permute(1, 2, 0)\n","    if rescale:\n","        plot_img = (plot_img - plot_img.min()) / (plot_img.max() - plot_img.min())\n","    return plot_img\n","\n","\n","def add_plot(writer, name, step):\n","    buf = io.BytesIO()\n","    plt.savefig(buf, format='jpeg', dpi=100)\n","    buf.seek(0)\n","    image = Image.open(buf)\n","    image = T.ToTensor()(image)\n","    writer.add_image(name, image, step)\n","    plt.clf()\n","    plt.close()\n","\n","\n","@torch.jit.script\n","def shuffle(x):\n","    return x[torch.randperm(x.shape[0])]\n","\n","\n","def add_hparams_fixed(writer, hparam_dict, metric_dict, global_step):\n","    exp, ssi, sei = hparams(hparam_dict, metric_dict)\n","    writer.file_writer.add_summary(exp)\n","    writer.file_writer.add_summary(ssi)\n","    writer.file_writer.add_summary(sei)\n","    for k, v in metric_dict.items():\n","        writer.add_scalar(k, v, global_step)\n","\n","\n","@torch.jit.script\n","def resize(classes: torch.Tensor, size: int):\n","    return F.interpolate(classes, (size, size), mode=\"bilinear\", align_corners=False)\n","\n","\n","def one_hot_feats(labels, n_classes):\n","    return F.one_hot(labels, n_classes).permute(0, 3, 1, 2).to(torch.float32)\n","\n","\n","def load_model(model_type, data_dir):\n","    if model_type == \"robust_resnet50\":\n","        model = models.resnet50(pretrained=False)\n","        model_file = join(data_dir, 'imagenet_l2_3_0.pt')\n","        if not os.path.exists(model_file):\n","            wget.download(\"http://6.869.csail.mit.edu/fa19/psets19/pset6/imagenet_l2_3_0.pt\",\n","                          model_file)\n","        model_weights = torch.load(model_file)\n","        model_weights_modified = {name.split('model.')[1]: value for name, value in model_weights['model'].items() if\n","                                  'model' in name}\n","        model.load_state_dict(model_weights_modified)\n","        model = nn.Sequential(*list(model.children())[:-1])\n","    elif model_type == \"densecl\":\n","        model = models.resnet50(pretrained=False)\n","        model_file = join(data_dir, 'densecl_r50_coco_1600ep.pth')\n","        if not os.path.exists(model_file):\n","            wget.download(\"https://cloudstor.aarnet.edu.au/plus/s/3GapXiWuVAzdKwJ/download\",\n","                          model_file)\n","        model_weights = torch.load(model_file)\n","        # model_weights_modified = {name.split('model.')[1]: value for name, value in model_weights['model'].items() if\n","        #                          'model' in name}\n","        model.load_state_dict(model_weights['state_dict'], strict=False)\n","        model = nn.Sequential(*list(model.children())[:-1])\n","    elif model_type == \"resnet50\":\n","        model = models.resnet50(pretrained=True)\n","        model = nn.Sequential(*list(model.children())[:-1])\n","    elif model_type == \"mocov2\":\n","        model = models.resnet50(pretrained=False)\n","        model_file = join(data_dir, 'moco_v2_800ep_pretrain.pth.tar')\n","        if not os.path.exists(model_file):\n","            wget.download(\"https://dl.fbaipublicfiles.com/moco/moco_checkpoints/\"\n","                          \"moco_v2_800ep/moco_v2_800ep_pretrain.pth.tar\", model_file)\n","        checkpoint = torch.load(model_file)\n","        # rename moco pre-trained keys\n","        state_dict = checkpoint['state_dict']\n","        for k in list(state_dict.keys()):\n","            # retain only encoder_q up to before the embedding layer\n","            if k.startswith('module.encoder_q') and not k.startswith('module.encoder_q.fc'):\n","                # remove prefix\n","                state_dict[k[len(\"module.encoder_q.\"):]] = state_dict[k]\n","            # delete renamed or unused k\n","            del state_dict[k]\n","        msg = model.load_state_dict(state_dict, strict=False)\n","        assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n","        model = nn.Sequential(*list(model.children())[:-1])\n","    elif model_type == \"densenet121\":\n","        model = models.densenet121(pretrained=True)\n","        model = nn.Sequential(*list(model.children())[:-1] + [nn.AdaptiveAvgPool2d((1, 1))])\n","    elif model_type == \"vgg11\":\n","        model = models.vgg11(pretrained=True)\n","        model = nn.Sequential(*list(model.children())[:-1] + [nn.AdaptiveAvgPool2d((1, 1))])\n","    else:\n","        raise ValueError(\"No model: {} found\".format(model_type))\n","\n","    model.eval()\n","    model.cuda()\n","    return model\n","\n","\n","class UnNormalize(object):\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, image):\n","        image2 = torch.clone(image)\n","        for t, m, s in zip(image2, self.mean, self.std):\n","            t.mul_(s).add_(m)\n","        return image2\n","\n","\n","normalize = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","unnorm = UnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","\n","\n","class ToTargetTensor(object):\n","    def __call__(self, target):\n","        return torch.as_tensor(np.array(target), dtype=torch.int64).unsqueeze(0)\n","\n","\n","def prep_args():\n","    import sys\n","\n","    old_args = sys.argv\n","    new_args = [old_args.pop(0)]\n","    while len(old_args) > 0:\n","        arg = old_args.pop(0)\n","        if len(arg.split(\"=\")) == 2:\n","            new_args.append(arg)\n","        elif arg.startswith(\"--\"):\n","            new_args.append(arg[2:] + \"=\" + old_args.pop(0))\n","        else:\n","            raise ValueError(\"Unexpected arg style {}\".format(arg))\n","    sys.argv = new_args\n","\n","\n","def get_transform(res, is_label, crop_type):\n","    if crop_type == \"center\":\n","        cropper = T.CenterCrop(res)\n","    elif crop_type == \"random\":\n","        cropper = T.RandomCrop(res)\n","    elif crop_type is None:\n","        cropper = T.Lambda(lambda x: x)\n","        res = (res, res)\n","    else:\n","        raise ValueError(\"Unknown Cropper {}\".format(crop_type))\n","    if is_label:\n","        return T.Compose([T.Resize(res, Image.NEAREST),\n","                          cropper,\n","                          ToTargetTensor()])\n","    else:\n","        return T.Compose([T.Resize(res, Image.NEAREST),\n","                          cropper,\n","                          T.ToTensor(),\n","                          normalize])\n","\n","\n","def _remove_axes(ax):\n","    ax.xaxis.set_major_formatter(plt.NullFormatter())\n","    ax.yaxis.set_major_formatter(plt.NullFormatter())\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","\n","\n","def remove_axes(axes):\n","    if len(axes.shape) == 2:\n","        for ax1 in axes:\n","            for ax in ax1:\n","                _remove_axes(ax)\n","    else:\n","        for ax in axes:\n","            _remove_axes(ax)\n","\n","\n","class UnsupervisedMetrics(Metric):\n","    def __init__(self, prefix: str, n_classes: int, extra_clusters: int, compute_hungarian: bool,\n","                 dist_sync_on_step=True):\n","        # call `self.add_state`for every internal state that is needed for the metrics computations\n","        # dist_reduce_fx indicates the function that should be used to reduce\n","        # state from multiple processes\n","        super().__init__(dist_sync_on_step=dist_sync_on_step)\n","\n","        self.n_classes = n_classes\n","        self.extra_clusters = extra_clusters\n","        self.compute_hungarian = compute_hungarian\n","        self.prefix = prefix\n","        self.add_state(\"stats\",\n","                       default=torch.zeros(n_classes + self.extra_clusters, n_classes, dtype=torch.int64),\n","                       dist_reduce_fx=\"sum\")\n","\n","    def update(self, preds: torch.Tensor, target: torch.Tensor):\n","        with torch.no_grad():\n","            actual = target.reshape(-1)\n","            preds = preds.reshape(-1)\n","            mask = (actual >= 0) & (actual < self.n_classes) & (preds >= 0) & (preds < self.n_classes)\n","            actual = actual[mask]\n","            preds = preds[mask]\n","            self.stats += torch.bincount(\n","                (self.n_classes + self.extra_clusters) * actual + preds,\n","                minlength=self.n_classes * (self.n_classes + self.extra_clusters)) \\\n","                .reshape(self.n_classes, self.n_classes + self.extra_clusters).t().to(self.stats.device)\n","\n","    def map_clusters(self, clusters):\n","        if self.extra_clusters == 0:\n","            return torch.tensor(self.assignments[1])[clusters]\n","        else:\n","            missing = sorted(list(set(range(self.n_classes + self.extra_clusters)) - set(self.assignments[0])))\n","            cluster_to_class = self.assignments[1]\n","            for missing_entry in missing:\n","                if missing_entry == cluster_to_class.shape[0]:\n","                    cluster_to_class = np.append(cluster_to_class, -1)\n","                else:\n","                    cluster_to_class = np.insert(cluster_to_class, missing_entry + 1, -1)\n","            cluster_to_class = torch.tensor(cluster_to_class)\n","            return cluster_to_class[clusters]\n","\n","    def compute(self):\n","        if self.compute_hungarian:\n","            self.assignments = linear_sum_assignment(self.stats.detach().cpu(), maximize=True)\n","            # print(self.assignments)\n","            if self.extra_clusters == 0:\n","                self.histogram = self.stats[np.argsort(self.assignments[1]), :]\n","            if self.extra_clusters > 0:\n","                self.assignments_t = linear_sum_assignment(self.stats.detach().cpu().t(), maximize=True)\n","                histogram = self.stats[self.assignments_t[1], :]\n","                missing = list(set(range(self.n_classes + self.extra_clusters)) - set(self.assignments[0]))\n","                new_row = self.stats[missing, :].sum(0, keepdim=True)\n","                histogram = torch.cat([histogram, new_row], axis=0)\n","                new_col = torch.zeros(self.n_classes + 1, 1, device=histogram.device)\n","                self.histogram = torch.cat([histogram, new_col], axis=1)\n","        else:\n","            self.assignments = (torch.arange(self.n_classes).unsqueeze(1),\n","                                torch.arange(self.n_classes).unsqueeze(1))\n","            self.histogram = self.stats\n","\n","        tp = torch.diag(self.histogram)\n","        fp = torch.sum(self.histogram, dim=0) - tp\n","        fn = torch.sum(self.histogram, dim=1) - tp\n","\n","        iou = tp / (tp + fp + fn)\n","        prc = tp / (tp + fn)\n","        opc = torch.sum(tp) / torch.sum(self.histogram)\n","\n","        metric_dict = {self.prefix + \"mIoU\": iou[~torch.isnan(iou)].mean().item(),\n","                       self.prefix + \"Accuracy\": opc.item()}\n","        return {k: 100 * v for k, v in metric_dict.items()}\n","\n","\n","def flexible_collate(batch):\n","    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n","\n","    elem = batch[0]\n","    elem_type = type(elem)\n","    if isinstance(elem, torch.Tensor):\n","        out = None\n","        if torch.utils.data.get_worker_info() is not None:\n","            # If we're in a background process, concatenate directly into a\n","            # shared memory tensor to avoid an extra copy\n","            numel = sum([x.numel() for x in batch])\n","            storage = elem.storage()._new_shared(numel)\n","            out = elem.new(storage)\n","        try:\n","            return torch.stack(batch, 0, out=out)\n","        except RuntimeError:\n","            return batch\n","    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n","            and elem_type.__name__ != 'string_':\n","        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n","            # array of string classes and object\n","            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n","                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n","\n","            return flexible_collate([torch.as_tensor(b) for b in batch])\n","        elif elem.shape == ():  # scalars\n","            return torch.as_tensor(batch)\n","    elif isinstance(elem, float):\n","        return torch.tensor(batch, dtype=torch.float64)\n","    elif isinstance(elem, int):\n","        return torch.tensor(batch)\n","    elif isinstance(elem, string_classes):\n","        return batch\n","    elif isinstance(elem, collections.abc.Mapping):\n","        return {key: flexible_collate([d[key] for d in batch]) for key in elem}\n","    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n","        return elem_type(*(flexible_collate(samples) for samples in zip(*batch)))\n","    elif isinstance(elem, collections.abc.Sequence):\n","        # check to make sure that the elements in batch have consistent size\n","        it = iter(batch)\n","        elem_size = len(next(it))\n","        if not all(len(elem) == elem_size for elem in it):\n","            raise RuntimeError('each element in list of batch should be of equal size')\n","        transposed = zip(*batch)\n","        return [flexible_collate(samples) for samples in transposed]\n","\n","    raise TypeError(default_collate_err_msg_format.format(elem_type))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T17:38:55.891193Z","iopub.status.busy":"2024-04-05T17:38:55.890861Z","iopub.status.idle":"2024-04-05T17:38:55.900895Z","shell.execute_reply":"2024-04-05T17:38:55.899676Z","shell.execute_reply.started":"2024-04-05T17:38:55.891169Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile crop_datasets.py\n","from modules import *\n","import os\n","from data import ContrastiveSegDataset\n","import hydra\n","import torch\n","from omegaconf import DictConfig, OmegaConf\n","from pytorch_lightning.utilities.seed import seed_everything\n","from torch.utils.data import DataLoader\n","from torchvision.transforms.functional import five_crop, get_image_size, crop\n","from tqdm import tqdm\n","from torch.utils.data import Dataset\n","\n","\n","def _random_crops(img, size, seed, n):\n","    \"\"\"Crop the given image into four corners and the central crop.\n","    If the image is torch Tensor, it is expected\n","    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n","\n","    .. Note::\n","        This transform returns a tuple of images and there may be a\n","        mismatch in the number of inputs and targets your ``Dataset`` returns.\n","\n","    Args:\n","        img (PIL Image or Tensor): Image to be cropped.\n","        size (sequence or int): Desired output size of the crop. If size is an\n","            int instead of sequence like (h, w), a square crop (size, size) is\n","            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n","\n","    Returns:\n","       tuple: tuple (tl, tr, bl, br, center)\n","                Corresponding top left, top right, bottom left, bottom right and center crop.\n","    \"\"\"\n","    if isinstance(size, int):\n","        size = (int(size), int(size))\n","    elif isinstance(size, (tuple, list)) and len(size) == 1:\n","        size = (size[0], size[0])\n","\n","    if len(size) != 2:\n","        raise ValueError(\"Please provide only two dimensions (h, w) for size.\")\n","\n","    image_width, image_height = get_image_size(img)\n","    crop_height, crop_width = size\n","    if crop_width > image_width or crop_height > image_height:\n","        msg = \"Requested crop size {} is bigger than input size {}\"\n","        raise ValueError(msg.format(size, (image_height, image_width)))\n","\n","    images = []\n","    for i in range(n):\n","        seed1 = hash((seed, i, 0))\n","        seed2 = hash((seed, i, 1))\n","        crop_height, crop_width = int(crop_height), int(crop_width)\n","\n","        top = seed1 % (image_height - crop_height)\n","        left = seed2 % (image_width - crop_width)\n","        images.append(crop(img, top, left, crop_height, crop_width))\n","\n","    return images\n","\n","\n","class RandomCropComputer(Dataset):\n","\n","    def _get_size(self, img):\n","        if len(img.shape) == 3:\n","            return [int(img.shape[1] * self.crop_ratio), int(img.shape[2] * self.crop_ratio)]\n","        elif len(img.shape) == 2:\n","            return [int(img.shape[0] * self.crop_ratio), int(img.shape[1] * self.crop_ratio)]\n","        else:\n","            raise ValueError(\"Bad image shape {}\".format(img.shape))\n","\n","    def random_crops(self, i, img):\n","        return _random_crops(img, self._get_size(img), i, 5)\n","\n","    def five_crops(self, i, img):\n","        return five_crop(img, self._get_size(img))\n","\n","    def __init__(self, cfg, dataset_name, img_set, crop_type, crop_ratio):\n","        self.pytorch_data_dir = cfg.pytorch_data_dir\n","        self.crop_ratio = crop_ratio\n","        self.save_dir = join(\n","            cfg.pytorch_data_dir, \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n","        self.img_set = img_set\n","        self.dataset_name = dataset_name\n","        self.cfg = cfg\n","\n","        self.img_dir = join(self.save_dir, \"img\", img_set)\n","        self.label_dir = join(self.save_dir, \"label\", img_set)\n","        os.makedirs(self.img_dir, exist_ok=True)\n","        os.makedirs(self.label_dir, exist_ok=True)\n","\n","        if crop_type == \"random\":\n","            cropper = lambda i, x: self.random_crops(i, x)\n","        elif crop_type == \"five\":\n","            cropper = lambda i, x: self.five_crops(i, x)\n","        else:\n","            raise ValueError('Unknown crop type {}'.format(crop_type))\n","\n","        self.dataset = ContrastiveSegDataset(\n","            cfg.pytorch_data_dir,\n","            dataset_name,\n","            None,\n","            img_set,\n","            T.ToTensor(),\n","            ToTargetTensor(),\n","            cfg=cfg,\n","            num_neighbors=cfg.num_neighbors,\n","            pos_labels=False,\n","            pos_images=False,\n","            mask=False,\n","            aug_geometric_transform=None,\n","            aug_photometric_transform=None,\n","            extra_transform=cropper\n","        )\n","\n","    def __getitem__(self, item):\n","        batch = self.dataset[item]\n","        imgs = batch['img']\n","        labels = batch['label']\n","        for crop_num, (img, label) in enumerate(zip(imgs, labels)):\n","            img_num = item * 5 + crop_num\n","            img_arr = img.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n","            label_arr = (label + 1).unsqueeze(0).permute(1, 2, 0).to('cpu', torch.uint8).numpy().squeeze(-1)\n","            Image.fromarray(img_arr).save(join(self.img_dir, \"{}.jpg\".format(img_num)), 'JPEG')\n","            Image.fromarray(label_arr).save(join(self.label_dir, \"{}.png\".format(img_num)), 'PNG')\n","        return True\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","\n","@hydra.main(config_path=\"configs\", config_name=\"train_config.yml\")\n","def my_app(cfg: DictConfig) -> None:\n","    print(OmegaConf.to_yaml(cfg))\n","    seed_everything(seed=0, workers=True)\n","\n","    # dataset_names = [\"cityscapes\", \"cocostuff27\"]\n","    # img_sets = [\"train\", \"val\"]\n","    # crop_types = [\"five\",\"random\"]\n","    # crop_ratios = [.5, .7]\n","\n","    dataset_names = [\"cityscapes\"]\n","    img_sets = [\"train\", \"val\"]\n","    crop_types = [\"five\"]\n","    crop_ratios = [.5]\n","\n","    for crop_ratio in crop_ratios:\n","        for crop_type in crop_types:\n","            for dataset_name in dataset_names:\n","                for img_set in img_sets:\n","                    dataset = RandomCropComputer(cfg, dataset_name, img_set, crop_type, crop_ratio)\n","                    loader = DataLoader(dataset, 1, shuffle=False, num_workers=cfg.num_workers, collate_fn=lambda l: l)\n","                    for _ in tqdm(loader):\n","                        pass\n","\n","\n","if __name__ == \"__main__\":\n","    prep_args()\n","    my_app()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T14:25:04.947043Z","iopub.status.busy":"2024-04-07T14:25:04.946708Z","iopub.status.idle":"2024-04-07T14:25:04.955213Z","shell.execute_reply":"2024-04-07T14:25:04.954339Z","shell.execute_reply.started":"2024-04-07T14:25:04.947019Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile precompute_knns.py\n","from data import ContrastiveSegDataset\n","from modules import *\n","import os\n","from os.path import join\n","import hydra\n","import numpy as np\n","import torch.multiprocessing\n","import torch.multiprocessing\n","import torch.nn as nn\n","from omegaconf import DictConfig, OmegaConf\n","from pytorch_lightning.utilities.seed import seed_everything\n","from tqdm import tqdm\n","\n","\n","def get_feats(model, loader):\n","    all_feats = []\n","    for pack in tqdm(loader):\n","        img = pack[\"img\"]\n","        feats = F.normalize(model.forward(img.cuda()).mean([2, 3]), dim=1)\n","        all_feats.append(feats.to(\"cpu\", non_blocking=True))\n","    return torch.cat(all_feats, dim=0).contiguous()\n","\n","\n","@hydra.main(config_path=\"configs\", config_name=\"train_config.yml\")\n","def my_app(cfg: DictConfig) -> None:\n","    print(OmegaConf.to_yaml(cfg))\n","    pytorch_data_dir = cfg.pytorch_data_dir\n","    data_dir = join(cfg.output_root, \"data\")\n","    log_dir = join(cfg.output_root, \"logs\")\n","    os.makedirs(data_dir, exist_ok=True)\n","    os.makedirs(log_dir, exist_ok=True)\n","    os.makedirs(join(pytorch_data_dir, \"nns\"), exist_ok=True)\n","\n","    seed_everything(seed=0)\n","\n","    print(data_dir)\n","    print(cfg.output_root)\n","\n","    image_sets = [\"val\", \"train\"]\n","#     dataset_names = [\"cocostuff27\", \"cityscapes\", \"potsdam\"]\n","#     dataset_names = [\"potsdam\"]\n","#     crop_types = [\"five\", None]\n","    dataset_names = [\"voc\"]\n","    crop_types = [None]\n","\n","    # Uncomment these lines to run on custom datasets\n","    #dataset_names = [\"directory\"]\n","    #crop_types = [None]\n","\n","    res = 224\n","    n_batches = 16\n","\n","    if cfg.arch == \"dino\":\n","        from modules import DinoFeaturizer, LambdaLayer\n","        no_ap_model = torch.nn.Sequential(\n","            DinoFeaturizer(20, cfg),  # dim doesent matter\n","            LambdaLayer(lambda p: p[0]),\n","        ).cuda()\n","    else:\n","        cut_model = load_model(cfg.model_type, join(cfg.output_root, \"data\")).cuda()\n","        no_ap_model = nn.Sequential(*list(cut_model.children())[:-1]).cuda()\n","    par_model = torch.nn.DataParallel(no_ap_model)\n","\n","    for crop_type in crop_types:\n","        for image_set in image_sets:\n","            for dataset_name in dataset_names:\n","                nice_dataset_name = cfg.dir_dataset_name if dataset_name == \"directory\" else dataset_name\n","\n","                feature_cache_file = join(pytorch_data_dir, \"nns\", \"nns_{}_{}_{}_{}_{}.npz\".format(\n","                    cfg.model_type, nice_dataset_name, image_set, crop_type, res))\n","\n","                if not os.path.exists(feature_cache_file):\n","                    print(\"{} not found, computing\".format(feature_cache_file))\n","                    dataset = ContrastiveSegDataset(\n","                        pytorch_data_dir=pytorch_data_dir,\n","                        dataset_name=dataset_name,\n","                        crop_type=crop_type,\n","                        image_set=image_set,\n","                        transform=get_transform(res, False, \"center\"),\n","                        target_transform=get_transform(res, True, \"center\"),\n","                        cfg=cfg,\n","                    )\n","\n","                    loader = DataLoader(dataset, 256, shuffle=False, num_workers=cfg.num_workers, pin_memory=False)\n","\n","                    with torch.no_grad():\n","                        normed_feats = get_feats(par_model, loader)\n","                        all_nns = []\n","                        step = normed_feats.shape[0] // n_batches\n","                        print(normed_feats.shape)\n","                        for i in tqdm(range(0, normed_feats.shape[0], step)):\n","                            torch.cuda.empty_cache()\n","                            batch_feats = normed_feats[i:i + step, :]\n","                            pairwise_sims = torch.einsum(\"nf,mf->nm\", batch_feats, normed_feats)\n","                            all_nns.append(torch.topk(pairwise_sims, 30)[1])\n","                            del pairwise_sims\n","                        nearest_neighbors = torch.cat(all_nns, dim=0)\n","\n","                        np.savez_compressed(feature_cache_file, nns=nearest_neighbors.numpy())\n","                        print(\"Saved NNs\", cfg.model_type, nice_dataset_name, image_set)\n","\n","\n","if __name__ == \"__main__\":\n","    prep_args()\n","    my_app()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T17:07:52.311957Z","iopub.status.busy":"2024-04-07T17:07:52.311116Z","iopub.status.idle":"2024-04-07T17:07:52.324930Z","shell.execute_reply":"2024-04-07T17:07:52.323979Z","shell.execute_reply.started":"2024-04-07T17:07:52.311920Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["%%writefile eval_segmentation.py\n","from modules import *\n","from data import *\n","from collections import defaultdict\n","from multiprocessing import Pool\n","import hydra\n","import seaborn as sns\n","import torch.multiprocessing\n","from crf import dense_crf\n","from omegaconf import DictConfig, OmegaConf\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from train_segmentation import LitUnsupervisedSegmenter, prep_for_plot, get_class_labels\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","print(\"imprt\\n\")\n","\n","class DummyPool:\n","    def map(self, func, iterable):\n","        return [func(args) for args in iterable]\n","\n","    def close(self):\n","        pass\n","\n","    def join(self):\n","        pass\n","    \n","    def __enter__(self):\n","        # Allows use with 'with' statement. Just returns itself.\n","        return self\n","\n","    def __exit__(self, exc_type, exc_val, exc_tb):\n","        # Context manager exit. No action needed for the dummy, but you could clean up resources here if needed.\n","        pass\n","\n","\n","def plot_cm(histogram, label_cmap, cfg):\n","    fig = plt.figure(figsize=(10, 10))\n","    ax = fig.gca()\n","    hist = histogram.detach().cpu().to(torch.float32)\n","    hist /= torch.clamp_min(hist.sum(dim=0, keepdim=True), 1)\n","    sns.heatmap(hist.t(), annot=False, fmt='g', ax=ax, cmap=\"Blues\", cbar=False)\n","    ax.set_title('Predicted labels', fontsize=28)\n","    ax.set_ylabel('True labels', fontsize=28)\n","    names = get_class_labels(cfg.dataset_name)\n","    if cfg.extra_clusters:\n","        names = names + [\"Extra\"]\n","    ax.set_xticks(np.arange(0, len(names)) + .5)\n","    ax.set_yticks(np.arange(0, len(names)) + .5)\n","    ax.xaxis.tick_top()\n","    ax.xaxis.set_ticklabels(names, fontsize=18)\n","    ax.yaxis.set_ticklabels(names, fontsize=18)\n","    colors = [label_cmap[i] / 255.0 for i in range(len(names))]\n","    [t.set_color(colors[i]) for i, t in enumerate(ax.xaxis.get_ticklabels())]\n","    [t.set_color(colors[i]) for i, t in enumerate(ax.yaxis.get_ticklabels())]\n","    plt.xticks(rotation=90)\n","    plt.yticks(rotation=0)\n","    ax.vlines(np.arange(0, len(names) + 1), color=[.5, .5, .5], *ax.get_xlim())\n","    ax.hlines(np.arange(0, len(names) + 1), color=[.5, .5, .5], *ax.get_ylim())\n","    plt.tight_layout()\n","\n","\n","def batch_list(iterable, n=1):\n","    l = len(iterable)\n","    for ndx in range(0, l, n):\n","        yield iterable[ndx:min(ndx + n, l)]\n","\n","\n","def _apply_crf(tup):\n","    return dense_crf(tup[0], tup[1])\n","    # import logging\n","    # try:\n","    #     logging.info(\"Processing _apply_crf for a tuple\")\n","    #     result = dense_crf(tup[0], tup[1])\n","    #     logging.info(\"Finished processing _apply_crf for a tuple\")\n","    #     return result\n","    # except Exception as e:\n","    #     logging.error(f\"Error in _apply_crf: {e}\")\n","\n","\n","def batched_crf(pool, img_tensor, prob_tensor):\n","    outputs = pool.map(_apply_crf, zip(img_tensor.detach().cpu(), prob_tensor.detach().cpu()))\n","    print(\"stuck here\\n\")\n","    return torch.cat([torch.from_numpy(arr).unsqueeze(0) for arr in outputs], dim=0)\n","\n","@hydra.main(config_path=\"configs\", config_name=\"eval_config.yml\")\n","def my_app(cfg: DictConfig) -> None:\n","    print(\"ckpt0\\n\")\n","    pytorch_data_dir = cfg.pytorch_data_dir\n","    result_dir = \"../results/predictions/{}\".format(cfg.experiment_name)\n","    os.makedirs(join(result_dir, \"img\"), exist_ok=True)\n","    os.makedirs(join(result_dir, \"label\"), exist_ok=True)\n","    os.makedirs(join(result_dir, \"cluster\"), exist_ok=True)\n","    os.makedirs(join(result_dir, \"picie\"), exist_ok=True)\n","    print(\"ckpt1\\n\")\n","    for model_path in cfg.model_paths:\n","        model = LitUnsupervisedSegmenter.load_from_checkpoint(model_path)\n","        print(\">???\\n\")\n","        print(OmegaConf.to_yaml(model.cfg))\n","        print(\"ckpt2\\n\")\n","        run_picie = cfg.run_picie and model.cfg.dataset_name == \"cocostuff27\"\n","        if run_picie:\n","            picie_state = torch.load(\"../saved_models/picie_and_probes.pth\")\n","            picie = picie_state[\"model\"].cuda()\n","            picie_cluster_probe = picie_state[\"cluster_probe\"].module.cuda()\n","            picie_cluster_metrics = picie_state[\"cluster_metrics\"]\n","        print(\"0.0\\n\")\n","        loader_crop = \"center\"\n","        test_dataset = ContrastiveSegDataset(\n","            pytorch_data_dir=pytorch_data_dir,\n","            dataset_name=model.cfg.dataset_name,\n","            crop_type=None,\n","            image_set=\"val\",\n","            transform=get_transform(cfg.res, False, loader_crop),\n","            target_transform=get_transform(cfg.res, True, loader_crop),\n","            cfg=model.cfg,\n","        )\n","        print(\"0.1\\n\")\n","        test_loader = DataLoader(test_dataset, cfg.batch_size * 2,\n","                                 shuffle=False, num_workers=cfg.num_workers,\n","                                 pin_memory=True, collate_fn=flexible_collate)\n","        print(\"0.2\\n\")\n","        model.eval().cuda()\n","\n","        if cfg.use_ddp:\n","            par_model = torch.nn.DataParallel(model.net)\n","            if run_picie:\n","                par_picie = torch.nn.DataParallel(picie)\n","        else:\n","            par_model = model.net\n","            if run_picie:\n","                par_picie = picie\n","        print(\"0.3\\n\")\n","        if model.cfg.dataset_name == \"cocostuff27\":\n","            # all_good_images = range(10)\n","            # all_good_images = range(250)\n","            # all_good_images = [61, 60, 49, 44, 13, 70] #Failure cases\n","            all_good_images = [19, 54, 67, 66, 65, 75, 77, 76, 124]  # Main figure\n","        elif model.cfg.dataset_name == \"cityscapes\":\n","            # all_good_images = range(80)\n","            # all_good_images = [ 5, 20, 56]\n","            all_good_images = [11, 32, 43, 52]\n","        else:\n","            # raise ValueError(\"Unknown Dataset {}\".format(model.cfg.dataset_name))\n","            all_good_images = range(10)\n","        batch_nums = torch.tensor([n // (cfg.batch_size * 2) for n in all_good_images])\n","        batch_offsets = torch.tensor([n % (cfg.batch_size * 2) for n in all_good_images])\n","\n","        print(\"1\\n\")\n","        saved_data = defaultdict(list)\n","        # with Pool(cfg.num_workers + 5) as pool:\n","        # with Pool(1) as pool:\n","        with DummyPool() as pool:\n","            for i, batch in enumerate(tqdm(test_loader)):\n","                with torch.no_grad():\n","                    img = batch[\"img\"].cuda()\n","                    label = batch[\"label\"].cuda()\n","                    print(\"1.1\\n\")\n","                    feats, code1 = par_model(img)\n","                    feats, code2 = par_model(img.flip(dims=[3]))\n","                    code = (code1 + code2.flip(dims=[3])) / 2\n","                    print(\"1.2\\n\")\n","                    code = F.interpolate(code, label.shape[-2:], mode='bilinear', align_corners=False)\n","                    print(\"1.2.1\\n\")\n","                    linear_probs = torch.log_softmax(model.linear_probe(code), dim=1)\n","                    cluster_probs = model.cluster_probe(code, 2, log_probs=True)\n","                    print(\"1.2.2\\n\")\n","                    if cfg.run_crf:\n","                        print(\"1.2.3\\n\")\n","                        linear_preds = batched_crf(pool, img, linear_probs).argmax(1).cuda()\n","                        print(\"1.2.4\\n\")\n","                        cluster_preds = batched_crf(pool, img, cluster_probs).argmax(1).cuda()\n","                    else:\n","                        linear_preds = linear_probs.argmax(1)\n","                        cluster_preds = cluster_probs.argmax(1)\n","                    print(\"1.3\\n\")\n","                    model.test_linear_metrics.update(linear_preds, label)\n","                    model.test_cluster_metrics.update(cluster_preds, label)\n","\n","                    if run_picie:\n","                        picie_preds = picie_cluster_metrics.map_clusters(\n","                            picie_cluster_probe(par_picie(img), None)[1].argmax(1).cpu())\n","                    print(\"1.4\\n\")\n","                    if i in batch_nums:\n","                        matching_offsets = batch_offsets[torch.where(batch_nums == i)]\n","                        for offset in matching_offsets:\n","                            saved_data[\"linear_preds\"].append(linear_preds.cpu()[offset].unsqueeze(0))\n","                            saved_data[\"cluster_preds\"].append(cluster_preds.cpu()[offset].unsqueeze(0))\n","                            saved_data[\"label\"].append(label.cpu()[offset].unsqueeze(0))\n","                            saved_data[\"img\"].append(img.cpu()[offset].unsqueeze(0))\n","                            print(\"1.5\\n\")\n","                            if run_picie:\n","                                saved_data[\"picie_preds\"].append(picie_preds.cpu()[offset].unsqueeze(0))\n","        saved_data = {k: torch.cat(v, dim=0) for k, v in saved_data.items()}\n","\n","        tb_metrics = {\n","            **model.test_linear_metrics.compute(),\n","            **model.test_cluster_metrics.compute(),\n","        }\n","\n","        print(\"\")\n","        print(model_path)\n","        print(tb_metrics)\n","\n","        if cfg.run_prediction:\n","            n_rows = 3\n","        else:\n","            n_rows = 2\n","\n","        if run_picie:\n","            n_rows += 1\n","\n","        if cfg.dark_mode:\n","            plt.style.use('dark_background')\n","\n","        print(\"2\\n\")\n","        for good_images in batch_list(range(len(all_good_images)), 10):\n","            fig, ax = plt.subplots(n_rows, len(good_images), figsize=(len(good_images) * 3, n_rows * 3))\n","            for i, img_num in enumerate(good_images):\n","                plot_img = (prep_for_plot(saved_data[\"img\"][img_num]) * 255).numpy().astype(np.uint8)\n","                plot_label = (model.label_cmap[saved_data[\"label\"][img_num]]).astype(np.uint8)\n","                Image.fromarray(plot_img).save(join(join(result_dir, \"img\", str(img_num) + \".jpg\")))\n","                Image.fromarray(plot_label).save(join(join(result_dir, \"label\", str(img_num) + \".png\")))\n","                # print(plot_img.shape)\n","                # print(img_num.shape)\n","                ax[0, i].imshow(plot_img)\n","                ax[1, i].imshow(plot_label)\n","                if cfg.run_prediction:\n","                    plot_cluster = (model.label_cmap[\n","                        model.test_cluster_metrics.map_clusters(\n","                            saved_data[\"cluster_preds\"][img_num])]) \\\n","                        .astype(np.uint8)\n","                    Image.fromarray(plot_cluster).save(join(join(result_dir, \"cluster\", str(img_num) + \".png\")))\n","                    ax[2, i].imshow(plot_cluster)\n","                if run_picie:\n","                    picie_img = model.label_cmap[saved_data[\"picie_preds\"][img_num]].astype(np.uint8)\n","                    ax[3, i].imshow(picie_img)\n","                    Image.fromarray(picie_img).save(join(join(result_dir, \"picie\", str(img_num) + \".png\")))\n","\n","            ax[0, 0].set_ylabel(\"Image\", fontsize=26)\n","            ax[1, 0].set_ylabel(\"Label\", fontsize=26)\n","            if cfg.run_prediction:\n","                ax[2, 0].set_ylabel(\"STEGO\\n(Ours)\", fontsize=26)\n","            if run_picie:\n","                ax[3, 0].set_ylabel(\"PiCIE\\n(Baseline)\", fontsize=26)\n","\n","            remove_axes(ax)\n","            plt.tight_layout()\n","            plt.show()\n","            plt.clf()\n","\n","        plot_cm(model.test_cluster_metrics.histogram, model.label_cmap, model.cfg)\n","        plt.show()\n","        plt.clf()\n","\n","\n","if __name__ == \"__main__\":\n","    prep_args()\n","    my_app()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile plot_dino_correspondence.py\n","import os\n","from os.path import join\n","from utils import get_transform, load_model, prep_for_plot, remove_axes, prep_args\n","from modules import FeaturePyramidNet, DinoFeaturizer, sample\n","from data import ContrastiveSegDataset\n","import hydra\n","import matplotlib.animation as animation\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from omegaconf import DictConfig, OmegaConf\n","from pytorch_lightning.utilities.seed import seed_everything\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from matplotlib.colors import ListedColormap\n","\n","\n","def plot_heatmap(ax, image, heatmap, cmap=\"bwr\", color=False, plot_img=True, symmetric=True):\n","    vmax = np.abs(heatmap).max()\n","    if not color:\n","        bw = np.dot(np.array(image)[..., :3], [0.2989, 0.5870, 0.1140])\n","        image = np.ones_like(image) * np.expand_dims(bw, -1)\n","\n","    if symmetric:\n","        kwargs = dict(vmax=vmax, vmin=-vmax)\n","    else:\n","        kwargs = {}\n","\n","    if plot_img:\n","        return [\n","            ax.imshow(image),\n","            ax.imshow(heatmap, alpha=.5, cmap=cmap, **kwargs),\n","        ]\n","    else:\n","        return [ax.imshow(heatmap, alpha=.5, cmap=cmap, **kwargs)]\n","\n","\n","def get_heatmaps(net, img, img_pos, query_points):\n","    feats1, _ = net(img.cuda())\n","    feats2, _ = net(img_pos.cuda())\n","\n","    sfeats1 = sample(feats1, query_points)\n","\n","    attn_intra = torch.einsum(\"nchw,ncij->nhwij\", F.normalize(sfeats1, dim=1), F.normalize(feats1, dim=1))\n","    attn_intra -= attn_intra.mean([3, 4], keepdims=True)\n","    attn_intra = attn_intra.clamp(0).squeeze(0)\n","\n","    attn_inter = torch.einsum(\"nchw,ncij->nhwij\", F.normalize(sfeats1, dim=1), F.normalize(feats2, dim=1))\n","    attn_inter -= attn_inter.mean([3, 4], keepdims=True)\n","    attn_inter = attn_inter.clamp(0).squeeze(0)\n","\n","    heatmap_intra = F.interpolate(\n","        attn_intra, img.shape[2:], mode=\"bilinear\", align_corners=True).squeeze(0).detach().cpu()\n","    heatmap_inter = F.interpolate(\n","        attn_inter, img_pos.shape[2:], mode=\"bilinear\", align_corners=True).squeeze(0).detach().cpu()\n","\n","    return heatmap_intra, heatmap_inter\n","\n","\n","@hydra.main(config_path=\"configs\", config_name=\"plot_config.yml\")\n","def my_app(cfg: DictConfig) -> None:\n","    print(OmegaConf.to_yaml(cfg))\n","    pytorch_data_dir = cfg.pytorch_data_dir\n","    data_dir = join(cfg.output_root, \"data\")\n","    log_dir = join(cfg.output_root, \"logs\")\n","    result_dir = join(cfg.output_root, \"results\", \"correspondence\")\n","    os.makedirs(data_dir, exist_ok=True)\n","    os.makedirs(log_dir, exist_ok=True)\n","    seed_everything(seed=0, workers=True)\n","    high_res = 512\n","\n","    transform = get_transform(high_res, False, \"center\")\n","    use_loader = True\n","\n","    if use_loader:\n","        dataset = ContrastiveSegDataset(\n","            pytorch_data_dir=pytorch_data_dir,\n","            dataset_name=cfg.dataset_name,\n","            crop_type=None,\n","            image_set=\"train\",\n","            transform=transform,\n","            target_transform=get_transform(high_res, True, \"center\"),\n","            cfg=cfg,\n","            aug_geometric_transform=None,\n","            aug_photometric_transform=None,\n","            num_neighbors=2,\n","            mask=True,\n","            pos_images=True,\n","            pos_labels=True,\n","        )\n","        loader = DataLoader(dataset, 16, shuffle=True, num_workers=cfg.num_workers)\n","\n","    data_dir = join(cfg.output_root, \"data\")\n","    if cfg.arch == \"feature-pyramid\":\n","        cut_model = load_model(cfg.model_type, data_dir).cuda()\n","        net = FeaturePyramidNet(cfg.granularity, cut_model, cfg.dim, cfg.continuous)\n","    elif cfg.arch == \"dino\":\n","        net = DinoFeaturizer(cfg.dim, cfg)\n","    else:\n","        raise ValueError(\"Unknown arch {}\".format(cfg.arch))\n","    net = net.cuda()\n","\n","    for batch_val in loader:\n","        batch = batch_val\n","        break\n","\n","    colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0)]\n","    cmaps = [\n","        ListedColormap([(1, 0, 0, i / 255) for i in range(255)]),\n","        ListedColormap([(0, 1, 0, i / 255) for i in range(255)]),\n","        ListedColormap([(0, 0, 1, i / 255) for i in range(255)]),\n","        ListedColormap([(1, 1, 0, i / 255) for i in range(255)])\n","    ]\n","\n","    with torch.no_grad():\n","        if cfg.plot_correspondence:\n","            img_num = 6\n","            query_points = torch.tensor(\n","                [\n","                    [-.1, 0.0],\n","                    [.5, .8],\n","                    [-.7, -.7],\n","                ]\n","            ).reshape(1, 3, 1, 2).cuda()\n","\n","            img = batch[\"img\"][img_num:img_num + 1]\n","            img_pos = batch[\"img_pos\"][img_num:img_num + 1]\n","\n","            plt.style.use('dark_background')\n","            fig, axes = plt.subplots(1, 3, figsize=(3 * 5, 1 * 5), dpi=100)\n","            remove_axes(axes)\n","            axes[0].set_title(\"Image and Query Points\", fontsize=20)\n","            axes[1].set_title(\"Self Correspondence\", fontsize=20)\n","            axes[2].set_title(\"KNN Correspondence\", fontsize=20)\n","            fig.tight_layout()\n","\n","            heatmap_intra, heatmap_inter = get_heatmaps(net, img, img_pos, query_points)\n","            for point_num in range(query_points.shape[1]):\n","                point = ((query_points[0, point_num, 0] + 1) / 2 * high_res).cpu()\n","                img_point_h = point[0]\n","                img_point_w = point[1]\n","\n","                plot_img = point_num == 0\n","                if plot_img:\n","                    axes[0].imshow(prep_for_plot(img[0]))\n","                axes[0].scatter(img_point_h, img_point_w,\n","                                c=colors[point_num], marker=\"x\", s=500, linewidths=5)\n","\n","                plot_heatmap(axes[1], prep_for_plot(img[0]) * .8, heatmap_intra[point_num],\n","                             plot_img=plot_img, cmap=cmaps[point_num], symmetric=False)\n","                plot_heatmap(axes[2], prep_for_plot(img_pos[0]) * .8, heatmap_inter[point_num],\n","                             plot_img=plot_img, cmap=cmaps[point_num], symmetric=False)\n","            plt.show()\n","\n","        if cfg.plot_movie:\n","            img_num = 6\n","            key_points = [\n","                [-.7, -.7],\n","                [-.1, 0.0],\n","                [.5, .8],\n","            ]\n","            all_points = []\n","            for i in range(len(key_points)):\n","                all_points.extend([key_points[i]] * 60)\n","\n","                if i < len(key_points) - 1:\n","                    all_points.extend(\n","                        np.stack([\n","                            np.linspace(key_points[i][0], key_points[i + 1][0], 50),\n","                            np.linspace(key_points[i][1], key_points[i + 1][1], 50),\n","                        ], axis=1).tolist())\n","            query_points = torch.tensor(all_points).reshape(1, len(all_points), 1, 2).cuda()\n","\n","\n","            plt.style.use('dark_background')\n","            fig, axes = plt.subplots(1, 3, figsize=(3 * 5, 1 * 5), dpi=100)\n","            remove_axes(axes)\n","            axes[0].set_title(\"Image and Query Points\", fontsize=20)\n","            axes[1].set_title(\"Self Correspondence\", fontsize=20)\n","            axes[2].set_title(\"KNN Correspondence\", fontsize=20)\n","\n","            fig.tight_layout()\n","\n","            heatmap_intra, heatmap_inter = get_heatmaps(net, img, img_pos, query_points)\n","\n","            frames = []  # for storing the generated images\n","            for point_num in range(query_points.shape[1]):\n","                point = ((query_points[0, point_num, 0] + 1) / 2 * high_res).cpu()\n","                img_point_h = point[0]\n","                img_point_w = point[1]\n","\n","                frame = []\n","\n","                frame.append(axes[0].imshow(prep_for_plot(img[0])))\n","\n","                frame.extend([\n","                    axes[0].scatter(img_point_h, img_point_w,\n","                                    c=colors[0], marker=\"x\", s=400, linewidths=4),\n","                    *plot_heatmap(axes[1], prep_for_plot(img[0]) * .8, heatmap_intra[point_num],\n","                                  cmap=cmaps[0], symmetric=False),\n","                    *plot_heatmap(axes[2], prep_for_plot(img_pos[0]) * .8, heatmap_inter[point_num],\n","                                  cmap=cmaps[0], symmetric=False)\n","                ])\n","\n","                frames.append(frame)\n","\n","            os.makedirs(result_dir, exist_ok=True)\n","\n","            with tqdm(total=len(frames)) as pbar:\n","                animation.ArtistAnimation(fig, frames, blit=True).save(\n","                    join(result_dir, 'attention_interp.mp4'),\n","                    progress_callback=lambda i, n: pbar.update(),\n","                    writer=animation.FFMpegWriter(fps=30))\n","\n","\n","if __name__ == \"__main__\":\n","    prep_args()\n","    my_app()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile plot_pr_curves.py\n","import io\n","from datetime import datetime\n","\n","import PIL.Image\n","import hydra\n","import pytorch_lightning as pl\n","import seaborn as sns\n","from omegaconf import DictConfig, OmegaConf\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from pytorch_lightning.utilities.seed import seed_everything\n","from sklearn.metrics import auc, precision_recall_curve, average_precision_score\n","from torch.utils.tensorboard.summary import hparams\n","from torchvision.transforms import ToTensor\n","from data import *\n","from modules import *\n","from train_segmentation import get_class_labels\n","\n","\n","\n","@torch.jit.script\n","def super_perm(size: int, device: torch.device):\n","    perm = torch.randperm(size, device=device, dtype=torch.long)\n","    perm[perm == torch.arange(size, device=device)] += 1\n","    return perm % size\n","\n","\n","def prep_fd_coord(fd):\n","    fd -= fd.mean([3, 4], keepdim=True)\n","    fd /= fd.std([3, 4], keepdim=True)\n","    return fd.reshape(-1)\n","\n","\n","def prep_fd(fd):\n","    fd -= fd.min()\n","    fd /= fd.max()\n","    return fd.reshape(-1)\n","\n","\n","def prep_fd_2(fd):\n","    fd -= fd.mean([3, 4], keepdim=True)\n","    fd -= fd.min()\n","    fd /= fd.max()\n","    return fd\n","\n","\n","def plot_auc_raw(name, fpr, tpr):\n","    fpr, tpr = fpr.detach().cpu().squeeze(), tpr.detach().cpu().squeeze()\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr, tpr, label=name + ' AUC = %0.2f' % roc_auc)\n","\n","\n","class CRFModule(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.w1 = torch.nn.Parameter(torch.tensor(10.), requires_grad=True)\n","        self.w2 = torch.nn.Parameter(torch.tensor(3.), requires_grad=True)\n","        self.shift = torch.nn.Parameter(torch.tensor(-.3), requires_grad=True)\n","        self.alpha = torch.nn.Parameter(torch.tensor(.5), requires_grad=True)\n","        self.beta = torch.nn.Parameter(torch.tensor(.15), requires_grad=True)\n","        self.gamma = torch.nn.Parameter(torch.tensor(.05), requires_grad=True)\n","\n","    def forward(self, coord_diff, img_diff):\n","        return torch.abs(self.w1) * torch.exp(- coord_diff / (2 * torch.exp(self.alpha))\n","                                              - img_diff / (2 * torch.exp(self.beta))) + \\\n","               torch.abs(self.w2) * torch.exp(- coord_diff / (2 * torch.exp(self.gamma))) - self.shift\n","\n","\n","class LitRecalibrator(pl.LightningModule):\n","    def __init__(self, n_classes, cfg):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.n_classes = n_classes\n","\n","        if not cfg.continuous:\n","            dim = n_classes\n","        else:\n","            dim = cfg.dim\n","\n","        data_dir = join(cfg.output_root, \"data\")\n","        self.moco = FeaturePyramidNet(cfg.granularity, load_model(\"mocov2\", data_dir).cuda(), dim, cfg.continuous)\n","        self.dino = DinoFeaturizer(dim, cfg)\n","        # self.dino = LitUnsupervisedSegmenter.load_from_checkpoint(\"../models/vit_base_cocostuff27.ckpt\").net\n","        self.crf = CRFModule()\n","        self.cm_metrics = UnsupervisedMetrics(\n","            \"confusion_matrix/\", n_classes, 0, False)\n","        self.automatic_optimization = False\n","\n","        if self.cfg.dataset_name.startswith(\"cityscapes\"):\n","            self.label_cmap = create_cityscapes_colormap()\n","        else:\n","            self.label_cmap = create_pascal_label_colormap()\n","\n","    def get_crf_fd(self, img, coords1, coords2):\n","        with torch.no_grad():\n","            n = img.shape[0]\n","            [h1, w1, h2, w2] = [self.cfg.feature_samples] * 4\n","            img_samples_1 = sample(img, coords1).permute(0, 2, 3, 1).reshape(n, -1, 1, 3)\n","            img_samples_2 = sample(img, coords2).permute(0, 2, 3, 1).reshape(n, 1, -1, 3)\n","            coord_diff = (coords1.reshape(n, -1, 1, 2) - coords2.reshape(n, 1, -1, 2)) \\\n","                .square().sum(-1).reshape(n, h1, w1, h2, w2)\n","\n","            img_diff = (img_samples_1 - img_samples_2).square().sum(-1).reshape(n, h1, w1, h2, w2)\n","\n","            return self.crf(coord_diff, img_diff)\n","\n","    def get_net_fd(self, feats1, feats2, label1, label2, coords1, coords2):\n","        with torch.no_grad():\n","            feat_samples1 = sample(feats1, coords1)\n","            feat_samples2 = sample(feats2, coords2)\n","\n","            label_samples1 = sample(F.one_hot(label1 + 1, self.n_classes + 1)\n","                                    .to(torch.float).permute(0, 3, 1, 2), coords1)\n","            label_samples2 = sample(F.one_hot(label2 + 1, self.n_classes + 1)\n","                                    .to(torch.float).permute(0, 3, 1, 2), coords2)\n","\n","            fd = tensor_correlation(norm(feat_samples1), norm(feat_samples2))\n","            ld = tensor_correlation(label_samples1, label_samples2)\n","\n","        return ld, fd, label_samples1.argmax(1), label_samples2.argmax(1)\n","\n","    def training_step(self, batch, batch_idx):\n","        return None\n","\n","    def validation_step(self, batch, batch_idx):\n","        with torch.no_grad():\n","            img = batch[\"img\"]\n","            label = batch[\"label\"]\n","\n","            dino_feats, dino_code = self.dino(img)\n","            moco_feats, moco_code = self.moco(img)\n","\n","            coord_shape = [img.shape[0], self.cfg.feature_samples, self.cfg.feature_samples, 2]\n","            coords1 = torch.rand(coord_shape, device=img.device) * 2 - 1\n","            coords2 = torch.rand(coord_shape, device=img.device) * 2 - 1\n","\n","            crf_fd = self.get_crf_fd(img, coords1, coords2)\n","\n","            ld, stego_fd, l1, l2 = self.get_net_fd(dino_code, dino_code, label, label, coords1, coords2)\n","            ld, dino_fd, l1, l2 = self.get_net_fd(dino_feats, dino_feats, label, label, coords1, coords2)\n","            ld, moco_fd, l1, l2 = self.get_net_fd(moco_feats, moco_feats, label, label, coords1, coords2)\n","\n","            return dict(\n","                dino_fd=dino_fd,\n","                stego_fd=stego_fd,\n","                moco_fd=moco_fd,\n","                crf_fd=crf_fd,\n","                ld=ld\n","            )\n","\n","    def validation_epoch_end(self, outputs) -> None:\n","        self.cm_metrics.compute()\n","\n","        all_outputs = {}\n","        for k in outputs[0].keys():\n","            t = torch.cat([o[k] for o in outputs], dim=0)\n","            all_outputs[k] = t\n","\n","        def plot_pr(preds, targets, name):\n","            preds = preds.cpu().reshape(-1)\n","            preds -= preds.min()\n","            preds /= preds.max()\n","            targets = targets.to(torch.int64).cpu().reshape(-1)\n","            precisions, recalls, _ = precision_recall_curve(targets, preds)\n","            average_precision = average_precision_score(targets, preds)\n","            plt.plot(recalls, precisions, label=\"AP={}% {}\".format(int(average_precision * 100), name))\n","\n","        def plot_cm():\n","            histogram = self.cm_metrics.histogram\n","            fig = plt.figure(figsize=(10, 10))\n","            ax = fig.gca()\n","            hist = histogram.detach().cpu().to(torch.float32)\n","            hist /= torch.clamp_min(hist.sum(dim=0, keepdim=True), 1)\n","            sns.heatmap(hist.t(), annot=False, fmt='g', ax=ax, cmap=\"Blues\", cbar=False)\n","            ax.set_title('KNN Labels', fontsize=28)\n","            ax.set_ylabel('Image labels', fontsize=28)\n","            names = get_class_labels(self.cfg.dataset_name)\n","            if self.cfg.extra_clusters:\n","                names = names + [\"Extra\"]\n","            ax.set_xticks(np.arange(0, len(names)) + .5)\n","            ax.set_yticks(np.arange(0, len(names)) + .5)\n","            ax.xaxis.tick_top()\n","            ax.xaxis.set_ticklabels(names, fontsize=18)\n","            ax.yaxis.set_ticklabels(names, fontsize=18)\n","            colors = [self.label_cmap[i] / 255.0 for i in range(len(names))]\n","            [t.set_color(colors[i]) for i, t in enumerate(ax.xaxis.get_ticklabels())]\n","            [t.set_color(colors[i]) for i, t in enumerate(ax.yaxis.get_ticklabels())]\n","            plt.xticks(rotation=90)\n","            plt.yticks(rotation=0)\n","            ax.vlines(np.arange(0, len(names) + 1), color=[.5, .5, .5], *ax.get_xlim())\n","            ax.hlines(np.arange(0, len(names) + 1), color=[.5, .5, .5], *ax.get_ylim())\n","            plt.tight_layout()\n","\n","        if self.trainer.is_global_zero:\n","            # plt.style.use('dark_background')\n","            print(\"Plotting\")\n","            plt.figure(figsize=(5, 4), dpi=100)\n","            plot_cm()\n","            plt.tight_layout()\n","            plt.show()\n","            plt.clf()\n","\n","            print(\"Plotting\")\n","            # plt.style.use('dark_background')\n","            plt.figure(figsize=(5, 4), dpi=100)\n","            ld = all_outputs[\"ld\"]\n","            plot_pr(prep_fd(all_outputs[\"stego_fd\"]), ld, \"STEGO (Ours)\")\n","            plot_pr(prep_fd(all_outputs[\"dino_fd\"]), ld, \"DINO\")\n","            plot_pr(prep_fd(all_outputs[\"moco_fd\"]), ld, \"MoCoV2\")\n","            plot_pr(prep_fd(all_outputs[\"crf_fd\"]), ld, \"CRF\")\n","            plt.xlim([0, 1])\n","            plt.ylim([0, 1])\n","            plt.legend(fontsize=12)\n","            plt.ylabel('Precision', fontsize=16)\n","            plt.xlabel('Recall', fontsize=16)\n","            plt.tight_layout()\n","            plt.show()\n","\n","        return None\n","\n","    def configure_optimizers(self):\n","        return None\n","\n","\n","@hydra.main(config_path=\"configs\", config_name=\"train_config.yml\")\n","def my_app(cfg: DictConfig) -> None:\n","    print(OmegaConf.to_yaml(cfg))\n","    pytorch_data_dir = cfg.pytorch_data_dir\n","    data_dir = join(cfg.output_root, \"data\")\n","    log_dir = join(cfg.output_root, \"logs\")\n","    checkpoint_dir = join(cfg.output_root, \"checkpoints\")\n","    os.makedirs(data_dir, exist_ok=True)\n","    os.makedirs(log_dir, exist_ok=True)\n","\n","    seed_everything(seed=0, workers=True)\n","\n","    train_dataset = ContrastiveSegDataset(\n","        pytorch_data_dir=pytorch_data_dir,\n","        dataset_name=cfg.dataset_name,\n","        crop_type=cfg.crop_type,\n","        image_set=\"train\",\n","        transform=get_transform(cfg.res, False, cfg.loader_crop_type),\n","        target_transform=get_transform(cfg.res, True, cfg.loader_crop_type),\n","        cfg=cfg,\n","        aug_geometric_transform=None,\n","        aug_photometric_transform=None,\n","        num_neighbors=cfg.num_neighbors,\n","        mask=True,\n","        pos_images=True,\n","        pos_labels=True\n","    )\n","\n","    val_loader_crop = \"center\"\n","    val_dataset = ContrastiveSegDataset(\n","        pytorch_data_dir=pytorch_data_dir,\n","        dataset_name=cfg.dataset_name,\n","        crop_type=None,\n","        image_set=\"val\",\n","        transform=get_transform(320, False, val_loader_crop),\n","        target_transform=get_transform(320, True, val_loader_crop),\n","        mask=True,\n","        pos_images=True,\n","        pos_labels=True,\n","        cfg=cfg,\n","    )\n","\n","    train_loader = DataLoader(train_dataset, cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n","    val_loader = DataLoader(val_dataset, cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n","\n","    model = LitRecalibrator(train_dataset.n_classes, cfg)\n","\n","    prefix = \"{}_{}\".format(cfg.dataset_name, cfg.experiment_name)\n","    name = '{}_date_{}'.format(prefix, datetime.now().strftime('%b%d_%H-%M-%S'))\n","    tb_logger = TensorBoardLogger(\n","        join(log_dir, cfg.log_dir, name),\n","        default_hp_metric=False\n","    )\n","    steps = 1\n","    trainer = Trainer(\n","        log_every_n_steps=10,\n","        val_check_interval=steps,\n","        gpus=1,\n","        max_steps=steps,\n","        limit_val_batches=100,\n","        accelerator=\"ddp\",\n","        num_sanity_val_steps=0,\n","        logger=tb_logger,\n","    )\n","    trainer.fit(model, train_loader, val_loader)\n","    os.makedirs(join(checkpoint_dir, cfg.log_dir), exist_ok=True)\n","\n","\n","if __name__ == \"__main__\":\n","    prep_args()\n","    my_app()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile data.py\n","import os\n","import random\n","from os.path import join\n","\n","import numpy as np\n","import torch.multiprocessing\n","from PIL import Image, ImageOps\n","from scipy.io import loadmat\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torchvision.datasets.cityscapes import Cityscapes\n","from torchvision.transforms.functional import to_pil_image\n","from tqdm import tqdm\n","\n","\n","def bit_get(val, idx):\n","    \"\"\"Gets the bit value.\n","    Args:\n","      val: Input value, int or numpy int array.\n","      idx: Which bit of the input val.\n","    Returns:\n","      The \"idx\"-th bit of input val.\n","    \"\"\"\n","    return (val >> idx) & 1\n","\n","\n","def create_pascal_label_colormap():\n","    \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n","    Returns:\n","      A colormap for visualizing segmentation results.\n","    \"\"\"\n","    colormap = np.zeros((512, 3), dtype=int)\n","    ind = np.arange(512, dtype=int)\n","\n","    for shift in reversed(list(range(8))):\n","        for channel in range(3):\n","            colormap[:, channel] |= bit_get(ind, channel) << shift\n","        ind >>= 3\n","\n","    return colormap\n","\n","\n","def create_cityscapes_colormap():\n","    colors = [(128, 64, 128),\n","              (244, 35, 232),\n","              (250, 170, 160),\n","              (230, 150, 140),\n","              (70, 70, 70),\n","              (102, 102, 156),\n","              (190, 153, 153),\n","              (180, 165, 180),\n","              (150, 100, 100),\n","              (150, 120, 90),\n","              (153, 153, 153),\n","              (153, 153, 153),\n","              (250, 170, 30),\n","              (220, 220, 0),\n","              (107, 142, 35),\n","              (152, 251, 152),\n","              (70, 130, 180),\n","              (220, 20, 60),\n","              (255, 0, 0),\n","              (0, 0, 142),\n","              (0, 0, 70),\n","              (0, 60, 100),\n","              (0, 0, 90),\n","              (0, 0, 110),\n","              (0, 80, 100),\n","              (0, 0, 230),\n","              (119, 11, 32),\n","              (0, 0, 0)]\n","    return np.array(colors)\n","\n","\n","class DirectoryDataset(Dataset):\n","    def __init__(self, root, path, image_set, transform, target_transform):\n","        super(DirectoryDataset, self).__init__()\n","        self.split = image_set\n","        self.dir = join(root, path)\n","        self.img_dir = join(self.dir, \"imgs\", self.split)\n","        self.label_dir = join(self.dir, \"labels\", self.split)\n","\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","        self.img_files = np.array(sorted(os.listdir(self.img_dir)))\n","        assert len(self.img_files) > 0\n","        if os.path.exists(join(self.dir, \"labels\")):\n","            self.label_files = np.array(sorted(os.listdir(self.label_dir)))\n","            assert len(self.img_files) == len(self.label_files)\n","        else:\n","            self.label_files = None\n","\n","    def __getitem__(self, index):\n","        image_fn = self.img_files[index]\n","        img = Image.open(join(self.img_dir, image_fn))\n","\n","        if self.label_files is not None:\n","            label_fn = self.label_files[index]\n","            label = Image.open(join(self.label_dir, label_fn))\n","\n","        seed = np.random.randint(2147483647)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        img = self.transform(img)\n","\n","        if self.label_files is not None:\n","            random.seed(seed)\n","            torch.manual_seed(seed)\n","            label = self.target_transform(label)\n","        else:\n","            label = torch.zeros(img.shape[1], img.shape[2], dtype=torch.int64) - 1\n","\n","        mask = (label > 0).to(torch.float32)\n","        return img, label, mask\n","\n","    def __len__(self):\n","        return len(self.img_files)\n","\n","\n","class Potsdam(Dataset):\n","    def __init__(self, root, image_set, transform, target_transform, coarse_labels):\n","        super(Potsdam, self).__init__()\n","        self.split = image_set\n","        self.root = os.path.join(root, \"potsdam\")\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        split_files = {\n","            \"train\": [\"labelled_train.txt\"],\n","            \"unlabelled_train\": [\"unlabelled_train.txt\"],\n","            # \"train\": [\"unlabelled_train.txt\"],\n","            \"val\": [\"labelled_test.txt\"],\n","            \"train+val\": [\"labelled_train.txt\", \"labelled_test.txt\"],\n","            \"all\": [\"all.txt\"]\n","        }\n","        assert self.split in split_files.keys()\n","\n","        self.files = []\n","        for split_file in split_files[self.split]:\n","            with open(join(self.root, split_file), \"r\") as f:\n","                self.files.extend(fn.rstrip() for fn in f.readlines())\n","\n","        self.coarse_labels = coarse_labels\n","        self.fine_to_coarse = {0: 0, 4: 0,  # roads and cars\n","                               1: 1, 5: 1,  # buildings and clutter\n","                               2: 2, 3: 2,  # vegetation and trees\n","                               255: -1\n","                               }\n","\n","    def __getitem__(self, index):\n","        image_id = self.files[index]\n","        img = loadmat(join(self.root, \"imgs\", image_id + \".mat\"))[\"img\"]\n","        img = to_pil_image(torch.from_numpy(img).permute(2, 0, 1)[:3])  # TODO add ir channel back\n","        try:\n","            label = loadmat(join(self.root, \"gt\", image_id + \".mat\"))[\"gt\"]\n","            label = to_pil_image(torch.from_numpy(label).unsqueeze(-1).permute(2, 0, 1))\n","        except FileNotFoundError:\n","            label = to_pil_image(torch.ones(1, img.height, img.width))\n","\n","        seed = np.random.randint(2147483647)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        img = self.transform(img)\n","\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        label = self.target_transform(label).squeeze(0)\n","        if self.coarse_labels:\n","            new_label_map = torch.zeros_like(label)\n","            for fine, coarse in self.fine_to_coarse.items():\n","                new_label_map[label == fine] = coarse\n","            label = new_label_map\n","\n","        mask = (label > 0).to(torch.float32)\n","        return img, label, mask\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","\n","class PotsdamRaw(Dataset):\n","    def __init__(self, root, image_set, transform, target_transform, coarse_labels):\n","        super(PotsdamRaw, self).__init__()\n","        self.split = image_set\n","        self.root = os.path.join(root, \"potsdamraw\", \"processed\")\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.files = []\n","        for im_num in range(38):\n","            for i_h in range(15):\n","                for i_w in range(15):\n","                    self.files.append(\"{}_{}_{}.mat\".format(im_num, i_h, i_w))\n","\n","        self.coarse_labels = coarse_labels\n","        self.fine_to_coarse = {0: 0, 4: 0,  # roads and cars\n","                               1: 1, 5: 1,  # buildings and clutter\n","                               2: 2, 3: 2,  # vegetation and trees\n","                               255: -1\n","                               }\n","\n","    def __getitem__(self, index):\n","        image_id = self.files[index]\n","        img = loadmat(join(self.root, \"imgs\", image_id))[\"img\"]\n","        img = to_pil_image(torch.from_numpy(img).permute(2, 0, 1)[:3])  # TODO add ir channel back\n","        try:\n","            label = loadmat(join(self.root, \"gt\", image_id))[\"gt\"]\n","            label = to_pil_image(torch.from_numpy(label).unsqueeze(-1).permute(2, 0, 1))\n","        except FileNotFoundError:\n","            label = to_pil_image(torch.ones(1, img.height, img.width))\n","\n","        seed = np.random.randint(2147483647)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        img = self.transform(img)\n","\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        label = self.target_transform(label).squeeze(0)\n","        if self.coarse_labels:\n","            new_label_map = torch.zeros_like(label)\n","            for fine, coarse in self.fine_to_coarse.items():\n","                new_label_map[label == fine] = coarse\n","            label = new_label_map\n","\n","        mask = (label > 0).to(torch.float32)\n","        return img, label, mask\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","\n","class Coco(Dataset):\n","    def __init__(self, root, image_set, transform, target_transform,\n","                 coarse_labels, exclude_things, subset=None):\n","        super(Coco, self).__init__()\n","        self.split = image_set\n","        self.root = join(root, \"cocostuff\")\n","        self.coarse_labels = coarse_labels\n","        self.transform = transform\n","        self.label_transform = target_transform\n","        self.subset = subset\n","        self.exclude_things = exclude_things\n","\n","        if self.subset is None:\n","            self.image_list = \"Coco164kFull_Stuff_Coarse.txt\"\n","        elif self.subset == 6:  # IIC Coarse\n","            self.image_list = \"Coco164kFew_Stuff_6.txt\"\n","        elif self.subset == 7:  # IIC Fine\n","            self.image_list = \"Coco164kFull_Stuff_Coarse_7.txt\"\n","\n","        assert self.split in [\"train\", \"val\", \"train+val\"]\n","        split_dirs = {\n","            \"train\": [\"train2017\"],\n","            \"val\": [\"val2017\"],\n","            \"train+val\": [\"train2017\", \"val2017\"]\n","        }\n","\n","        self.image_files = []\n","        self.label_files = []\n","        for split_dir in split_dirs[self.split]:\n","            with open(join(self.root, \"curated\", split_dir, self.image_list), \"r\") as f:\n","                img_ids = [fn.rstrip() for fn in f.readlines()]\n","                for img_id in img_ids:\n","                    self.image_files.append(join(self.root, \"images\", split_dir, img_id + \".jpg\"))\n","                    self.label_files.append(join(self.root, \"annotations\", split_dir, img_id + \".png\"))\n","\n","        self.fine_to_coarse = {0: 9, 1: 11, 2: 11, 3: 11, 4: 11, 5: 11, 6: 11, 7: 11, 8: 11, 9: 8, 10: 8, 11: 8, 12: 8,\n","                               13: 8, 14: 8, 15: 7, 16: 7, 17: 7, 18: 7, 19: 7, 20: 7, 21: 7, 22: 7, 23: 7, 24: 7,\n","                               25: 6, 26: 6, 27: 6, 28: 6, 29: 6, 30: 6, 31: 6, 32: 6, 33: 10, 34: 10, 35: 10, 36: 10,\n","                               37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 5, 44: 5, 45: 5, 46: 5, 47: 5, 48: 5,\n","                               49: 5, 50: 5, 51: 2, 52: 2, 53: 2, 54: 2, 55: 2, 56: 2, 57: 2, 58: 2, 59: 2, 60: 2,\n","                               61: 3, 62: 3, 63: 3, 64: 3, 65: 3, 66: 3, 67: 3, 68: 3, 69: 3, 70: 3, 71: 0, 72: 0,\n","                               73: 0, 74: 0, 75: 0, 76: 0, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 4, 84: 4,\n","                               85: 4, 86: 4, 87: 4, 88: 4, 89: 4, 90: 4, 91: 17, 92: 17, 93: 22, 94: 20, 95: 20, 96: 22,\n","                               97: 15, 98: 25, 99: 16, 100: 13, 101: 12, 102: 12, 103: 17, 104: 17, 105: 23, 106: 15,\n","                               107: 15, 108: 17, 109: 15, 110: 21, 111: 15, 112: 25, 113: 13, 114: 13, 115: 13, 116: 13,\n","                               117: 13, 118: 22, 119: 26, 120: 14, 121: 14, 122: 15, 123: 22, 124: 21, 125: 21, 126: 24,\n","                               127: 20, 128: 22, 129: 15, 130: 17, 131: 16, 132: 15, 133: 22, 134: 24, 135: 21, 136: 17,\n","                               137: 25, 138: 16, 139: 21, 140: 17, 141: 22, 142: 16, 143: 21, 144: 21, 145: 25, 146: 21,\n","                               147: 26, 148: 21, 149: 24, 150: 20, 151: 17, 152: 14, 153: 21, 154: 26, 155: 15, 156: 23,\n","                               157: 20, 158: 21, 159: 24, 160: 15, 161: 24, 162: 22, 163: 25, 164: 15, 165: 20, 166: 17,\n","                               167: 17, 168: 22, 169: 14, 170: 18, 171: 18, 172: 18, 173: 18, 174: 18, 175: 18, 176: 18,\n","                               177: 26, 178: 26, 179: 19, 180: 19, 181: 24}\n","\n","        self._label_names = [\n","            \"ground-stuff\",\n","            \"plant-stuff\",\n","            \"sky-stuff\",\n","        ]\n","        self.cocostuff3_coarse_classes = [23, 22, 21]\n","        self.first_stuff_index = 12\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_files[index]\n","        label_path = self.label_files[index]\n","        seed = np.random.randint(2147483647)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        img = self.transform(Image.open(image_path).convert(\"RGB\"))\n","\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        label = self.label_transform(Image.open(label_path)).squeeze(0)\n","        label[label == 255] = -1  # to be consistent with 10k\n","        coarse_label = torch.zeros_like(label)\n","        for fine, coarse in self.fine_to_coarse.items():\n","            coarse_label[label == fine] = coarse\n","        coarse_label[label == -1] = -1\n","\n","        if self.coarse_labels:\n","            coarser_labels = -torch.ones_like(label)\n","            for i, c in enumerate(self.cocostuff3_coarse_classes):\n","                coarser_labels[coarse_label == c] = i\n","            return img, coarser_labels, coarser_labels >= 0\n","        else:\n","            if self.exclude_things:\n","                return img, coarse_label - self.first_stuff_index, (coarse_label >= self.first_stuff_index)\n","            else:\n","                return img, coarse_label, coarse_label >= 0\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","class Pascal(Dataset):\n","    def __init__(self, root, image_set, transform, target_transform):\n","        super(Pascal, self).__init__()\n","        self.split = image_set\n","        self.root = join(root, \"VOC2012\")\n","        self.transform = transform\n","        self.label_transform = target_transform\n","\n","        assert self.split in [\"train\", \"val\", \"train+val\"]\n","        split_dirs = {\n","            \"train\": [\"ImageSets/Segmentation/train.txt\"],\n","            \"val\": [\"ImageSets/Segmentation/val.txt\"],\n","            \"train+val\": [\"ImageSets/Segmentation/trainval.txt\"]\n","        }\n","\n","        self.image_files = []\n","        self.label_files = []\n","        for split_dir in split_dirs[self.split]:\n","            with open(join(self.root, split_dir), \"r\") as f:\n","                img_ids = [fn.rstrip() for fn in f.readlines()]\n","                for img_id in img_ids:\n","                    self.image_files.append(join(self.root, \"JPEGImages\", img_id + \".jpg\"))\n","                    self.label_files.append(join(self.root, \"Annotations\", img_id + \".png\"))\n","        self.first_stuff_index = 12\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_files[index]\n","        label_path = self.label_files[index]\n","        seed = np.random.randint(2147483647)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        img = self.transform(Image.open(image_path).convert(\"RGB\"))\n","\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        label = self.label_transform(ImageOps.grayscale(Image.open(label_path))).squeeze(0)\n","        label[label == 255] = -1  # Dont know if need this\n","\n","        return img, label, label >= 0\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","class CityscapesSeg(Dataset):\n","    def __init__(self, root, image_set, transform, target_transform):\n","        super(CityscapesSeg, self).__init__()\n","        self.split = image_set\n","        self.root = join(root, \"cityscapes\")\n","        if image_set == \"train\":\n","            # our_image_set = \"train_extra\"\n","            # mode = \"coarse\"\n","            our_image_set = \"train\"\n","            mode = \"fine\"\n","        else:\n","            our_image_set = image_set\n","            mode = \"fine\"\n","        self.inner_loader = Cityscapes(self.root, our_image_set,\n","                                       mode=mode,\n","                                       target_type=\"semantic\",\n","                                       transform=None,\n","                                       target_transform=None)\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.first_nonvoid = 7\n","\n","    def __getitem__(self, index):\n","        if self.transform is not None:\n","            image, target = self.inner_loader[index]\n","\n","            seed = np.random.randint(2147483647)\n","            random.seed(seed)\n","            torch.manual_seed(seed)\n","            image = self.transform(image)\n","            random.seed(seed)\n","            torch.manual_seed(seed)\n","            target = self.target_transform(target)\n","\n","            target = target - self.first_nonvoid\n","            target[target < 0] = -1\n","            mask = target == -1\n","            return image, target.squeeze(0), mask\n","        else:\n","            return self.inner_loader[index]\n","\n","    def __len__(self):\n","        return len(self.inner_loader)\n","\n","\n","class CroppedDataset(Dataset):\n","    def __init__(self, root, dataset_name, crop_type, crop_ratio, image_set, transform, target_transform):\n","        super(CroppedDataset, self).__init__()\n","        self.dataset_name = dataset_name\n","        self.split = image_set\n","        self.root = join(root, \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.img_dir = join(self.root, \"img\", self.split)\n","        self.label_dir = join(self.root, \"label\", self.split)\n","        self.num_images = len(os.listdir(self.img_dir))\n","        assert self.num_images == len(os.listdir(self.label_dir))\n","\n","    def __getitem__(self, index):\n","        image = Image.open(join(self.img_dir, \"{}.jpg\".format(index))).convert('RGB')\n","        target = Image.open(join(self.label_dir, \"{}.png\".format(index)))\n","\n","        seed = np.random.randint(2147483647)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        image = self.transform(image)\n","        random.seed(seed)\n","        torch.manual_seed(seed)\n","        target = self.target_transform(target)\n","\n","        target = target - 1\n","        mask = target == -1\n","        return image, target.squeeze(0), mask\n","\n","    def __len__(self):\n","        return self.num_images\n","\n","\n","class MaterializedDataset(Dataset):\n","\n","    def __init__(self, ds):\n","        self.ds = ds\n","        self.materialized = []\n","        loader = DataLoader(ds, num_workers=12, collate_fn=lambda l: l[0])\n","        for batch in tqdm(loader):\n","            self.materialized.append(batch)\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, ind):\n","        return self.materialized[ind]\n","\n","\n","class ContrastiveSegDataset(Dataset):\n","    def __init__(self,\n","                 pytorch_data_dir,\n","                 dataset_name,\n","                 crop_type,\n","                 image_set,\n","                 transform,\n","                 target_transform,\n","                 cfg,\n","                 aug_geometric_transform=None,\n","                 aug_photometric_transform=None,\n","                 num_neighbors=5,\n","                 compute_knns=False,\n","                 mask=False,\n","                 pos_labels=False,\n","                 pos_images=False,\n","                 extra_transform=None,\n","                 model_type_override=None\n","                 ):\n","        super(ContrastiveSegDataset).__init__()\n","        self.num_neighbors = num_neighbors\n","        self.image_set = image_set\n","        self.dataset_name = dataset_name\n","        self.mask = mask\n","        self.pos_labels = pos_labels\n","        self.pos_images = pos_images\n","        self.extra_transform = extra_transform\n","\n","        if dataset_name == \"potsdam\":\n","            self.n_classes = 3\n","            dataset_class = Potsdam\n","            extra_args = dict(coarse_labels=True)\n","        elif dataset_name == \"potsdamraw\":\n","            self.n_classes = 3\n","            dataset_class = PotsdamRaw\n","            extra_args = dict(coarse_labels=True)\n","        elif dataset_name == \"directory\":\n","            self.n_classes = cfg.dir_dataset_n_classes\n","            dataset_class = DirectoryDataset\n","            extra_args = dict(path=cfg.dir_dataset_name)\n","        elif dataset_name == \"cityscapes\" and crop_type is None:\n","            self.n_classes = 27\n","            dataset_class = CityscapesSeg\n","            extra_args = dict()\n","        elif dataset_name == \"cityscapes\" and crop_type is not None:\n","            self.n_classes = 27\n","            dataset_class = CroppedDataset\n","            extra_args = dict(dataset_name=\"cityscapes\", crop_type=crop_type, crop_ratio=cfg.crop_ratio)\n","        elif dataset_name == \"cocostuff3\":\n","            self.n_classes = 3\n","            dataset_class = Coco\n","            extra_args = dict(coarse_labels=True, subset=6, exclude_things=True)\n","        elif dataset_name == \"cocostuff15\":\n","            self.n_classes = 15\n","            dataset_class = Coco\n","            extra_args = dict(coarse_labels=False, subset=7, exclude_things=True)\n","        elif dataset_name == \"cocostuff27\" and crop_type is not None:\n","            self.n_classes = 27\n","            dataset_class = CroppedDataset\n","            extra_args = dict(dataset_name=\"cocostuff27\", crop_type=cfg.crop_type, crop_ratio=cfg.crop_ratio)\n","        elif dataset_name == \"cocostuff27\" and crop_type is None:\n","            self.n_classes = 27\n","            dataset_class = Coco\n","            extra_args = dict(coarse_labels=False, subset=None, exclude_things=False)\n","            if image_set == \"val\":\n","                extra_args[\"subset\"] = 7\n","#         elif dataset_name == \"PascalVOC2012\":\n","        elif dataset_name == \"voc\":\n","            self.n_classes = 21\n","            dataset_class = Pascal\n","            extra_args = dict()\n","        else:\n","            raise ValueError(\"Unknown dataset: {}\".format(dataset_name))\n","\n","        self.aug_geometric_transform = aug_geometric_transform\n","        self.aug_photometric_transform = aug_photometric_transform\n","\n","        self.dataset = dataset_class(\n","            root=pytorch_data_dir,\n","            image_set=self.image_set,\n","            transform=transform,\n","            target_transform=target_transform, **extra_args)\n","\n","        if model_type_override is not None:\n","            model_type = model_type_override\n","        else:\n","            model_type = cfg.model_type\n","\n","        nice_dataset_name = cfg.dir_dataset_name if dataset_name == \"directory\" else dataset_name\n","        feature_cache_file = join(pytorch_data_dir, \"nns\", \"nns_{}_{}_{}_{}_{}.npz\".format(\n","            model_type, nice_dataset_name, image_set, crop_type, cfg.res))\n","        if pos_labels or pos_images:\n","            if not os.path.exists(feature_cache_file) or compute_knns:\n","                raise ValueError(\"could not find nn file {} please run precompute_knns\".format(feature_cache_file))\n","            else:\n","                loaded = np.load(feature_cache_file)\n","                self.nns = loaded[\"nns\"]\n","            assert len(self.dataset) == self.nns.shape[0]\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def _set_seed(self, seed):\n","        random.seed(seed)  # apply this seed to img tranfsorms\n","        torch.manual_seed(seed)  # needed for torchvision 0.7\n","\n","    def __getitem__(self, ind):\n","        pack = self.dataset[ind]\n","\n","        if self.pos_images or self.pos_labels:\n","            ind_pos = self.nns[ind][torch.randint(low=1, high=self.num_neighbors + 1, size=[]).item()]\n","            pack_pos = self.dataset[ind_pos]\n","\n","        seed = np.random.randint(2147483647)  # make a seed with numpy generator\n","\n","        self._set_seed(seed)\n","        coord_entries = torch.meshgrid([torch.linspace(-1, 1, pack[0].shape[1]),\n","                                        torch.linspace(-1, 1, pack[0].shape[2])])\n","        coord = torch.cat([t.unsqueeze(0) for t in coord_entries], 0)\n","\n","        if self.extra_transform is not None:\n","            extra_trans = self.extra_transform\n","        else:\n","            extra_trans = lambda i, x: x\n","\n","        ret = {\n","            \"ind\": ind,\n","            \"img\": extra_trans(ind, pack[0]),\n","            \"label\": extra_trans(ind, pack[1]),\n","        }\n","\n","        if self.pos_images:\n","            ret[\"img_pos\"] = extra_trans(ind, pack_pos[0])\n","            ret[\"ind_pos\"] = ind_pos\n","\n","        if self.mask:\n","            ret[\"mask\"] = pack[2]\n","\n","        if self.pos_labels:\n","            ret[\"label_pos\"] = extra_trans(ind, pack_pos[1])\n","            ret[\"mask_pos\"] = pack_pos[2]\n","\n","        if self.aug_photometric_transform is not None:\n","            img_aug = self.aug_photometric_transform(self.aug_geometric_transform(pack[0]))\n","\n","            self._set_seed(seed)\n","            coord_aug = self.aug_geometric_transform(coord)\n","\n","            ret[\"img_aug\"] = img_aug\n","            ret[\"coord_aug\"] = coord_aug.permute(1, 2, 0)\n","\n","        return ret"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile train_segmentation.py\n","from utils import *\n","from modules import *\n","from data import *\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from datetime import datetime\n","import hydra\n","from omegaconf import DictConfig, OmegaConf\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from pytorch_lightning.utilities.seed import seed_everything\n","import torch.multiprocessing\n","import seaborn as sns\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import sys\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","def get_class_labels(dataset_name):\n","    if dataset_name.startswith(\"cityscapes\"):\n","        return [\n","            'road', 'sidewalk', 'parking', 'rail track', 'building',\n","            'wall', 'fence', 'guard rail', 'bridge', 'tunnel',\n","            'pole', 'polegroup', 'traffic light', 'traffic sign', 'vegetation',\n","            'terrain', 'sky', 'person', 'rider', 'car',\n","            'truck', 'bus', 'caravan', 'trailer', 'train',\n","            'motorcycle', 'bicycle']\n","    elif dataset_name == \"cocostuff27\":\n","        return [\n","            \"electronic\", \"appliance\", \"food\", \"furniture\", \"indoor\",\n","            \"kitchen\", \"accessory\", \"animal\", \"outdoor\", \"person\",\n","            \"sports\", \"vehicle\", \"ceiling\", \"floor\", \"food\",\n","            \"furniture\", \"rawmaterial\", \"textile\", \"wall\", \"window\",\n","            \"building\", \"ground\", \"plant\", \"sky\", \"solid\",\n","            \"structural\", \"water\"]\n","    elif dataset_name == \"voc\":\n","        return [\n","            'background',\n","            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n","            'bus', 'car', 'cat', 'chair', 'cow',\n","            'diningtable', 'dog', 'horse', 'motorbike', 'person',\n","            'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n","    elif dataset_name == \"potsdam\":\n","        return [\n","            'roads and cars',\n","            'buildings and clutter',\n","            'trees and vegetation']\n","    else:\n","        raise ValueError(\"Unknown Dataset {}\".format(dataset_name))\n","\n","\n","class LitUnsupervisedSegmenter(pl.LightningModule):\n","    def __init__(self, n_classes, cfg):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.n_classes = n_classes\n","\n","        if not cfg.continuous:\n","            dim = n_classes\n","        else:\n","            dim = cfg.dim\n","\n","        data_dir = join(cfg.output_root, \"data\")\n","        if cfg.arch == \"feature-pyramid\":\n","            cut_model = load_model(cfg.model_type, data_dir).cuda()\n","            self.net = FeaturePyramidNet(cfg.granularity, cut_model, dim, cfg.continuous)\n","        elif cfg.arch == \"dino\":\n","            self.net = DinoFeaturizer(dim, cfg)\n","        else:\n","            raise ValueError(\"Unknown arch {}\".format(cfg.arch))\n","\n","        self.train_cluster_probe = ClusterLookup(dim, n_classes)\n","\n","        self.cluster_probe = ClusterLookup(dim, n_classes + cfg.extra_clusters)\n","        self.linear_probe = nn.Conv2d(dim, n_classes, (1, 1))\n","\n","        self.decoder = nn.Conv2d(dim, self.net.n_feats, (1, 1))\n","\n","        self.cluster_metrics = UnsupervisedMetrics(\n","            \"test/cluster/\", n_classes, cfg.extra_clusters, True)\n","        self.linear_metrics = UnsupervisedMetrics(\n","            \"test/linear/\", n_classes, 0, False)\n","\n","        self.test_cluster_metrics = UnsupervisedMetrics(\n","            \"final/cluster/\", n_classes, cfg.extra_clusters, True)\n","        self.test_linear_metrics = UnsupervisedMetrics(\n","            \"final/linear/\", n_classes, 0, False)\n","\n","        self.linear_probe_loss_fn = torch.nn.CrossEntropyLoss()\n","        self.crf_loss_fn = ContrastiveCRFLoss(\n","            cfg.crf_samples, cfg.alpha, cfg.beta, cfg.gamma, cfg.w1, cfg.w2, cfg.shift)\n","\n","        self.contrastive_corr_loss_fn = ContrastiveCorrelationLoss(cfg)\n","        for p in self.contrastive_corr_loss_fn.parameters():\n","            p.requires_grad = False\n","\n","        self.automatic_optimization = False\n","\n","        if self.cfg.dataset_name.startswith(\"cityscapes\"):\n","            self.label_cmap = create_cityscapes_colormap()\n","        else:\n","            self.label_cmap = create_pascal_label_colormap()\n","\n","        self.val_steps = 0\n","        self.save_hyperparameters()\n","\n","    def forward(self, x):\n","        # in lightning, forward defines the prediction/inference actions\n","        return self.net(x)[1]\n","\n","    def training_step(self, batch, batch_idx):\n","        # training_step defined the train loop.\n","        # It is independent of forward\n","        net_optim, linear_probe_optim, cluster_probe_optim = self.optimizers()\n","\n","        net_optim.zero_grad()\n","        linear_probe_optim.zero_grad()\n","        cluster_probe_optim.zero_grad()\n","\n","        with torch.no_grad():\n","            ind = batch[\"ind\"]\n","            img = batch[\"img\"]\n","            img_aug = batch[\"img_aug\"]\n","            coord_aug = batch[\"coord_aug\"]\n","            img_pos = batch[\"img_pos\"]\n","            label = batch[\"label\"]\n","            label_pos = batch[\"label_pos\"]\n","\n","        feats, code = self.net(img)\n","        if self.cfg.correspondence_weight > 0:\n","            feats_pos, code_pos = self.net(img_pos)\n","        log_args = dict(sync_dist=False, rank_zero_only=True)\n","\n","        if self.cfg.use_true_labels:\n","            signal = one_hot_feats(label + 1, self.n_classes + 1)\n","            signal_pos = one_hot_feats(label_pos + 1, self.n_classes + 1)\n","        else:\n","            signal = feats\n","            signal_pos = feats_pos\n","\n","        loss = 0\n","\n","        should_log_hist = (self.cfg.hist_freq is not None) and \\\n","                          (self.global_step % self.cfg.hist_freq == 0) and \\\n","                          (self.global_step > 0)\n","        if self.cfg.use_salience:\n","            salience = batch[\"mask\"].to(torch.float32).squeeze(1)\n","            salience_pos = batch[\"mask_pos\"].to(torch.float32).squeeze(1)\n","        else:\n","            salience = None\n","            salience_pos = None\n","\n","        if self.cfg.correspondence_weight > 0:\n","            (\n","                pos_intra_loss, pos_intra_cd,\n","                pos_inter_loss, pos_inter_cd,\n","                neg_inter_loss, neg_inter_cd,\n","            ) = self.contrastive_corr_loss_fn(\n","                signal, signal_pos,\n","                salience, salience_pos,\n","                code, code_pos,\n","            )\n","\n","            if should_log_hist:\n","                self.logger.experiment.add_histogram(\"intra_cd\", pos_intra_cd, self.global_step)\n","                self.logger.experiment.add_histogram(\"inter_cd\", pos_inter_cd, self.global_step)\n","                self.logger.experiment.add_histogram(\"neg_cd\", neg_inter_cd, self.global_step)\n","            neg_inter_loss = neg_inter_loss.mean()\n","            pos_intra_loss = pos_intra_loss.mean()\n","            pos_inter_loss = pos_inter_loss.mean()\n","            self.log('loss/pos_intra', pos_intra_loss, **log_args)\n","            self.log('loss/pos_inter', pos_inter_loss, **log_args)\n","            self.log('loss/neg_inter', neg_inter_loss, **log_args)\n","            self.log('cd/pos_intra', pos_intra_cd.mean(), **log_args)\n","            self.log('cd/pos_inter', pos_inter_cd.mean(), **log_args)\n","            self.log('cd/neg_inter', neg_inter_cd.mean(), **log_args)\n","\n","            loss += (self.cfg.pos_inter_weight * pos_inter_loss +\n","                     self.cfg.pos_intra_weight * pos_intra_loss +\n","                     self.cfg.neg_inter_weight * neg_inter_loss) * self.cfg.correspondence_weight\n","\n","        if self.cfg.rec_weight > 0:\n","            rec_feats = self.decoder(code)\n","            rec_loss = -(norm(rec_feats) * norm(feats)).sum(1).mean()\n","            self.log('loss/rec', rec_loss, **log_args)\n","            loss += self.cfg.rec_weight * rec_loss\n","\n","        if self.cfg.aug_alignment_weight > 0:\n","            orig_feats_aug, orig_code_aug = self.net(img_aug)\n","            downsampled_coord_aug = resize(\n","                coord_aug.permute(0, 3, 1, 2),\n","                orig_code_aug.shape[2]).permute(0, 2, 3, 1)\n","            aug_alignment = -torch.einsum(\n","                \"bkhw,bkhw->bhw\",\n","                norm(sample(code, downsampled_coord_aug)),\n","                norm(orig_code_aug)\n","            ).mean()\n","            self.log('loss/aug_alignment', aug_alignment, **log_args)\n","            loss += self.cfg.aug_alignment_weight * aug_alignment\n","\n","        if self.cfg.crf_weight > 0:\n","            crf = self.crf_loss_fn(\n","                resize(img, 56),\n","                norm(resize(code, 56))\n","            ).mean()\n","            self.log('loss/crf', crf, **log_args)\n","            loss += self.cfg.crf_weight * crf\n","\n","        flat_label = label.reshape(-1)\n","        mask = (flat_label >= 0) & (flat_label < self.n_classes)\n","\n","        detached_code = torch.clone(code.detach())\n","\n","        linear_logits = self.linear_probe(detached_code)\n","        linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n","        linear_logits = linear_logits.permute(0, 2, 3, 1).reshape(-1, self.n_classes)\n","        linear_loss = self.linear_probe_loss_fn(linear_logits[mask], flat_label[mask]).mean()\n","        loss += linear_loss\n","        self.log('loss/linear', linear_loss, **log_args)\n","\n","        cluster_loss, cluster_probs = self.cluster_probe(detached_code, None)\n","        loss += cluster_loss\n","        self.log('loss/cluster', cluster_loss, **log_args)\n","        self.log('loss/total', loss, **log_args)\n","\n","        self.manual_backward(loss)\n","        net_optim.step()\n","        cluster_probe_optim.step()\n","        linear_probe_optim.step()\n","\n","        if self.cfg.reset_probe_steps is not None and self.global_step == self.cfg.reset_probe_steps:\n","            print(\"RESETTING PROBES\")\n","            self.linear_probe.reset_parameters()\n","            self.cluster_probe.reset_parameters()\n","            self.trainer.optimizers[1] = torch.optim.Adam(list(self.linear_probe.parameters()), lr=5e-3)\n","            self.trainer.optimizers[2] = torch.optim.Adam(list(self.cluster_probe.parameters()), lr=5e-3)\n","\n","        if self.global_step % 2000 == 0 and self.global_step > 0:\n","            print(\"RESETTING TFEVENT FILE\")\n","            # Make a new tfevent file\n","            self.logger.experiment.close()\n","            self.logger.experiment._get_file_writer()\n","\n","        return loss\n","\n","    def on_train_start(self):\n","        tb_metrics = {\n","            **self.linear_metrics.compute(),\n","            **self.cluster_metrics.compute()\n","        }\n","        self.logger.log_hyperparams(self.cfg, tb_metrics)\n","\n","    def validation_step(self, batch, batch_idx):\n","        img = batch[\"img\"]\n","        label = batch[\"label\"]\n","        self.net.eval()\n","\n","        with torch.no_grad():\n","            feats, code = self.net(img)\n","            code = F.interpolate(code, label.shape[-2:], mode='bilinear', align_corners=False)\n","\n","            linear_preds = self.linear_probe(code)\n","            linear_preds = linear_preds.argmax(1)\n","            self.linear_metrics.update(linear_preds, label)\n","\n","            cluster_loss, cluster_preds = self.cluster_probe(code, None)\n","            cluster_preds = cluster_preds.argmax(1)\n","            self.cluster_metrics.update(cluster_preds, label)\n","\n","            return {\n","                'img': img[:self.cfg.n_images].detach().cpu(),\n","                'linear_preds': linear_preds[:self.cfg.n_images].detach().cpu(),\n","                \"cluster_preds\": cluster_preds[:self.cfg.n_images].detach().cpu(),\n","                \"label\": label[:self.cfg.n_images].detach().cpu()}\n","\n","    def validation_epoch_end(self, outputs) -> None:\n","        super().validation_epoch_end(outputs)\n","        with torch.no_grad():\n","            tb_metrics = {\n","                **self.linear_metrics.compute(),\n","                **self.cluster_metrics.compute(),\n","            }\n","\n","            if self.trainer.is_global_zero and not self.cfg.submitting_to_aml:\n","                #output_num = 0\n","                output_num = random.randint(0, len(outputs) -1)\n","                output = {k: v.detach().cpu() for k, v in outputs[output_num].items()}\n","\n","                fig, ax = plt.subplots(4, self.cfg.n_images, figsize=(self.cfg.n_images * 3, 4 * 3))\n","                for i in range(self.cfg.n_images):\n","                    ax[0, i].imshow(prep_for_plot(output[\"img\"][i]))\n","                    ax[1, i].imshow(self.label_cmap[output[\"label\"][i]])\n","                    ax[2, i].imshow(self.label_cmap[output[\"linear_preds\"][i]])\n","                    ax[3, i].imshow(self.label_cmap[self.cluster_metrics.map_clusters(output[\"cluster_preds\"][i])])\n","                ax[0, 0].set_ylabel(\"Image\", fontsize=16)\n","                ax[1, 0].set_ylabel(\"Label\", fontsize=16)\n","                ax[2, 0].set_ylabel(\"Linear Probe\", fontsize=16)\n","                ax[3, 0].set_ylabel(\"Cluster Probe\", fontsize=16)\n","                remove_axes(ax)\n","                plt.tight_layout()\n","                add_plot(self.logger.experiment, \"plot_labels\", self.global_step)\n","\n","                if self.cfg.has_labels:\n","                    fig = plt.figure(figsize=(13, 10))\n","                    ax = fig.gca()\n","                    hist = self.cluster_metrics.histogram.detach().cpu().to(torch.float32)\n","                    hist /= torch.clamp_min(hist.sum(dim=0, keepdim=True), 1)\n","                    sns.heatmap(hist.t(), annot=False, fmt='g', ax=ax, cmap=\"Blues\")\n","                    ax.set_xlabel('Predicted labels')\n","                    ax.set_ylabel('True labels')\n","                    names = get_class_labels(self.cfg.dataset_name)\n","                    if self.cfg.extra_clusters:\n","                        names = names + [\"Extra\"]\n","                    ax.set_xticks(np.arange(0, len(names)) + .5)\n","                    ax.set_yticks(np.arange(0, len(names)) + .5)\n","                    ax.xaxis.tick_top()\n","                    ax.xaxis.set_ticklabels(names, fontsize=14)\n","                    ax.yaxis.set_ticklabels(names, fontsize=14)\n","                    colors = [self.label_cmap[i] / 255.0 for i in range(len(names))]\n","                    [t.set_color(colors[i]) for i, t in enumerate(ax.xaxis.get_ticklabels())]\n","                    [t.set_color(colors[i]) for i, t in enumerate(ax.yaxis.get_ticklabels())]\n","                    # ax.yaxis.get_ticklabels()[-1].set_color(self.label_cmap[0] / 255.0)\n","                    # ax.xaxis.get_ticklabels()[-1].set_color(self.label_cmap[0] / 255.0)\n","                    plt.xticks(rotation=90)\n","                    plt.yticks(rotation=0)\n","                    ax.vlines(np.arange(0, len(names) + 1), color=[.5, .5, .5], *ax.get_xlim())\n","                    ax.hlines(np.arange(0, len(names) + 1), color=[.5, .5, .5], *ax.get_ylim())\n","                    plt.tight_layout()\n","                    add_plot(self.logger.experiment, \"conf_matrix\", self.global_step)\n","\n","                    all_bars = torch.cat([\n","                        self.cluster_metrics.histogram.sum(0).cpu(),\n","                        self.cluster_metrics.histogram.sum(1).cpu()\n","                    ], axis=0)\n","                    ymin = max(all_bars.min() * .8, 1)\n","                    ymax = all_bars.max() * 1.2\n","\n","                    fig, ax = plt.subplots(1, 2, figsize=(2 * 5, 1 * 4))\n","                    ax[0].bar(range(self.n_classes + self.cfg.extra_clusters),\n","                              self.cluster_metrics.histogram.sum(0).cpu(),\n","                              tick_label=names,\n","                              color=colors)\n","                    ax[0].set_ylim(ymin, ymax)\n","                    ax[0].set_title(\"Label Frequency\")\n","                    ax[0].set_yscale('log')\n","                    ax[0].tick_params(axis='x', labelrotation=90)\n","\n","                    ax[1].bar(range(self.n_classes + self.cfg.extra_clusters),\n","                              self.cluster_metrics.histogram.sum(1).cpu(),\n","                              tick_label=names,\n","                              color=colors)\n","                    ax[1].set_ylim(ymin, ymax)\n","                    ax[1].set_title(\"Cluster Frequency\")\n","                    ax[1].set_yscale('log')\n","                    ax[1].tick_params(axis='x', labelrotation=90)\n","\n","                    plt.tight_layout()\n","                    add_plot(self.logger.experiment, \"label frequency\", self.global_step)\n","\n","            if self.global_step > 2:\n","                self.log_dict(tb_metrics)\n","\n","                if self.trainer.is_global_zero and self.cfg.azureml_logging:\n","                    from azureml.core.run import Run\n","                    run_logger = Run.get_context()\n","                    for metric, value in tb_metrics.items():\n","                        run_logger.log(metric, value)\n","\n","            self.linear_metrics.reset()\n","            self.cluster_metrics.reset()\n","\n","    def configure_optimizers(self):\n","        main_params = list(self.net.parameters())\n","\n","        if self.cfg.rec_weight > 0:\n","            main_params.extend(self.decoder.parameters())\n","\n","        net_optim = torch.optim.Adam(main_params, lr=self.cfg.lr)\n","        linear_probe_optim = torch.optim.Adam(list(self.linear_probe.parameters()), lr=5e-3)\n","        cluster_probe_optim = torch.optim.Adam(list(self.cluster_probe.parameters()), lr=5e-3)\n","\n","        return net_optim, linear_probe_optim, cluster_probe_optim\n","\n","\n","@hydra.main(config_path=\"configs\", config_name=\"train_config.yml\")\n","def my_app(cfg: DictConfig) -> None:\n","    OmegaConf.set_struct(cfg, False)\n","    print(OmegaConf.to_yaml(cfg))\n","    pytorch_data_dir = cfg.pytorch_data_dir\n","    data_dir = join(cfg.output_root, \"data\")\n","    log_dir = join(cfg.output_root, \"logs\")\n","    checkpoint_dir = join(cfg.output_root, \"checkpoints\")\n","\n","    prefix = \"{}/{}_{}\".format(cfg.log_dir, cfg.dataset_name, cfg.experiment_name)\n","    name = '{}_date_{}'.format(prefix, datetime.now().strftime('%b%d_%H-%M-%S'))\n","    cfg.full_name = prefix\n","\n","    os.makedirs(data_dir, exist_ok=True)\n","    os.makedirs(log_dir, exist_ok=True)\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","    seed_everything(seed=0)\n","\n","    print(data_dir)\n","    print(cfg.output_root)\n","\n","    geometric_transforms = T.Compose([\n","        T.RandomHorizontalFlip(),\n","        T.RandomResizedCrop(size=cfg.res, scale=(0.8, 1.0))\n","    ])\n","    photometric_transforms = T.Compose([\n","        T.ColorJitter(brightness=.3, contrast=.3, saturation=.3, hue=.1),\n","        T.RandomGrayscale(.2),\n","        T.RandomApply([T.GaussianBlur((5, 5))])\n","    ])\n","\n","    sys.stdout.flush()\n","\n","    train_dataset = ContrastiveSegDataset(\n","        pytorch_data_dir=pytorch_data_dir,\n","        dataset_name=cfg.dataset_name,\n","        crop_type=cfg.crop_type,\n","        image_set=\"train\",\n","        transform=get_transform(cfg.res, False, cfg.loader_crop_type),\n","        target_transform=get_transform(cfg.res, True, cfg.loader_crop_type),\n","        cfg=cfg,\n","        aug_geometric_transform=geometric_transforms,\n","        aug_photometric_transform=photometric_transforms,\n","        num_neighbors=cfg.num_neighbors,\n","        mask=True,\n","        pos_images=True,\n","        pos_labels=True\n","    )\n","\n","    if cfg.dataset_name == \"voc\":\n","        val_loader_crop = None\n","    else:\n","        val_loader_crop = \"center\"\n","\n","    val_dataset = ContrastiveSegDataset(\n","        pytorch_data_dir=pytorch_data_dir,\n","        dataset_name=cfg.dataset_name,\n","        crop_type=None,\n","        image_set=\"val\",\n","        transform=get_transform(320, False, val_loader_crop),\n","        target_transform=get_transform(320, True, val_loader_crop),\n","        mask=True,\n","        cfg=cfg,\n","    )\n","\n","    #val_dataset = MaterializedDataset(val_dataset)\n","    train_loader = DataLoader(train_dataset, cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n","\n","    if cfg.submitting_to_aml:\n","        val_batch_size = 16\n","    else:\n","        val_batch_size = cfg.batch_size\n","\n","    val_loader = DataLoader(val_dataset, val_batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n","\n","    model = LitUnsupervisedSegmenter(train_dataset.n_classes, cfg)\n","\n","    tb_logger = TensorBoardLogger(\n","        join(log_dir, name),\n","        default_hp_metric=False\n","    )\n","\n","    if cfg.submitting_to_aml:\n","        gpu_args = dict(gpus=1, val_check_interval=250)\n","\n","        if gpu_args[\"val_check_interval\"] > len(train_loader):\n","            gpu_args.pop(\"val_check_interval\")\n","\n","    else:\n","        gpu_args = dict(gpus=-1, accelerator='ddp', val_check_interval=cfg.val_freq)\n","        # gpu_args = dict(gpus=1, accelerator='ddp', val_check_interval=cfg.val_freq)\n","\n","        if gpu_args[\"val_check_interval\"] > len(train_loader) // 4:\n","            gpu_args.pop(\"val_check_interval\")\n","\n","    trainer = Trainer(\n","        log_every_n_steps=cfg.scalar_log_freq,\n","        logger=tb_logger,\n","        max_steps=cfg.max_steps,\n","        max_epochs=1000,\n","        callbacks=[\n","            ModelCheckpoint(\n","                dirpath=join(checkpoint_dir, name),\n","                every_n_train_steps=200, #400,\n","#                 save_top_k=2,\n","                save_top_k=2,#-1\n","                monitor=\"test/cluster/mIoU\",\n","                mode=\"max\",\n","            )\n","        ],\n","        **gpu_args\n","    )\n","    trainer.fit(model, train_loader, val_loader)\n","\n","\n","if __name__ == \"__main__\":\n","    prep_args()\n","    my_app()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 5. plot backbone, train and evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# optional, only necessary if you want to check the performance of the DINO backbone\n","# !python plot_dino_correspondence.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-05T17:47:10.750960Z","iopub.status.busy":"2024-04-05T17:47:10.749988Z","iopub.status.idle":"2024-04-05T17:48:01.429753Z","shell.execute_reply":"2024-04-05T17:48:01.428530Z","shell.execute_reply.started":"2024-04-05T17:47:10.750912Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!python precompute_knns.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-07T14:25:59.957080Z","iopub.status.busy":"2024-04-07T14:25:59.956751Z","iopub.status.idle":"2024-04-07T16:59:50.850640Z","shell.execute_reply":"2024-04-07T16:59:50.849672Z","shell.execute_reply.started":"2024-04-07T14:25:59.957055Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!python train_segmentation.py"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python eval_segmentation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-05T21:05:30.515141Z","iopub.status.busy":"2024-04-05T21:05:30.514782Z","iopub.status.idle":"2024-04-05T21:07:18.550519Z","shell.execute_reply":"2024-04-05T21:07:18.549388Z","shell.execute_reply.started":"2024-04-05T21:05:30.515113Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# optional, for PR curve, but I don't know why the code they provide only plots blank figures. Maybe warinings are useful.\n","# !python plot_pr_curves.py"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4697686,"sourceId":7981427,"sourceType":"datasetVersion"},{"datasetId":4734142,"sourceId":8031695,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
